{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X11g7MK9MWET"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **Systolic Blood Pressure Regression**\n",
        "In *this project*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "from numpy.linalg import inv\n",
        "from numpy.random import RandomState"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1a. Shuffle the datalist before splitting the data\n",
        "\n",
        "1b. Training dataset accounts for 75% of the datalist\n",
        "\n",
        "1c. Validation dataset accounts for 25% of the datalist"
      ],
      "metadata": {
        "id": "tVVW80-HVGcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData(training_datalist):\n",
        "  training_data = training_datalist[1:]\n",
        "  training_dataset = training_data[:int((len(training_data)+1)*.75)]\n",
        "  validation_dataset = training_data[int((len(training_data)+1)*.75):]\n",
        "  return training_dataset, validation_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset, validation_dataset = SplitData(training_datalist)"
      ],
      "metadata": {
        "id": "Kv1ycmasRlhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(training_datalist, training_dataset, validation_dataset):\n",
        "  x_training = [eval(row[0]) for row in training_datalist[1:]]\n",
        "  sort_x = np.sort(x_training)\n",
        "  training_x_q1 = np.percentile(sort_x, 25, interpolation = 'midpoint')\n",
        "  training_x_q3 = np.percentile(sort_x, 75, interpolation = 'midpoint')\n",
        "  training_x_iqr = training_x_q3 - training_x_q1\n",
        "  training_x_low_lim = training_x_q1 - 1.5 * training_x_iqr\n",
        "  training_x_up_lim = training_x_q3 + 1.5 * training_x_iqr\n",
        "\n",
        "  y_training = [eval(row[1]) for row in training_datalist[1:]]\n",
        "  sort_y = np.sort(y_training)\n",
        "  training_y_q1 = np.percentile(sort_y, 25, interpolation = 'midpoint')\n",
        "  training_y_q3 = np.percentile(sort_y, 75, interpolation = 'midpoint')\n",
        "  training_y_iqr = training_y_q3 - training_y_q1\n",
        "  training_y_low_lim = training_y_q1 - 1.5 * training_y_iqr\n",
        "  training_y_up_lim = training_y_q3 + 1.5 * training_y_iqr\n",
        "\n",
        "  training_data = []\n",
        "  for x, y in training_dataset:\n",
        "    if((eval(x) >= training_x_low_lim and eval(x) <= training_x_up_lim) and (eval(y) >= training_y_low_lim and eval(y) <= training_y_up_lim)):\n",
        "      training_data.append([eval(x), eval(y)])\n",
        "\n",
        "  validation_data = []\n",
        "  for x, y in validation_dataset:\n",
        "    if((eval(x) >= training_x_low_lim and eval(x) <= training_x_up_lim) and (eval(y) >= training_y_low_lim and eval(y) <= training_y_up_lim)):\n",
        "      validation_data.append([eval(x), eval(y)])\n",
        "\n",
        "  return np.array(training_data), np.array(validation_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data = PreprocessData(training_datalist, training_dataset, validation_dataset)"
      ],
      "metadata": {
        "id": "QAZK2QuasNPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion(training_data):\n",
        "  y = np.array([row[1] for row in training_data])\n",
        "  X = np.array([[1, row[0]] for row in training_data])\n",
        "  X_transpose = X.transpose()\n",
        "  X_XT = np.matmul(X_transpose, X)\n",
        "  X_XT_inv = inv(X_XT)\n",
        "  X_XT_inv_XT = np.matmul(X_XT_inv, X_transpose)\n",
        "  beta = np.matmul(X_XT_inv_XT, y)\n",
        "  return beta\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta = MatrixInversion(training_data)"
      ],
      "metadata": {
        "id": "n8FoD2A0sigR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(beta, data):\n",
        "  predicted_y = beta[1] * data + beta[0]\n",
        "  return predicted_y\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = MakePrediction(beta, testing_datalist[1:,0].astype(float))"
      ],
      "metadata": {
        "id": "gCiJGIbM-8h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in prediction:\n",
        "  output_datalist.append([data])\n"
      ],
      "metadata": {
        "id": "yL-T7tkx_HDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCL92EPKOFIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed29aeb4-c3fe-49d0-eb80-470b6716d0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9466749053269127 51.35975616774732\n"
          ]
        }
      ],
      "source": [
        "print(f'{beta[1]} {beta[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ],
      "metadata": {
        "id": "ImJMVMX7royU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData(training_datalist):\n",
        "  training_data = training_datalist[1:]\n",
        "  np.random.shuffle(training_data)\n",
        "  training_dataset = training_data[:int((len(training_data)+1)*.75)]\n",
        "  validation_dataset = training_data[int((len(training_data)+1)*.75):]\n",
        "  return training_dataset, validation_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset, validation_dataset = SplitData(training_datalist)"
      ],
      "metadata": {
        "id": "OlpGN7yHX3Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData(training_datalist, training_dataset, validation_dataset):\n",
        "  x_training = [eval(row[0]) for row in training_datalist[1:]]\n",
        "  sort_x = np.sort(x_training)\n",
        "  training_x_q1 = np.percentile(sort_x, 25, interpolation = 'midpoint')\n",
        "  training_x_q3 = np.percentile(sort_x, 75, interpolation = 'midpoint')\n",
        "  training_x_iqr = training_x_q3 - training_x_q1\n",
        "  training_x_low_lim = training_x_q1 - 1.5 * training_x_iqr\n",
        "  training_x_up_lim = training_x_q3 + 1.5 * training_x_iqr\n",
        "\n",
        "  y_training = [eval(row[1]) for row in training_datalist[1:]]\n",
        "  sort_y = np.sort(y_training)\n",
        "  training_y_q1 = np.percentile(sort_y, 25, interpolation = 'midpoint')\n",
        "  training_y_q3 = np.percentile(sort_y, 75, interpolation = 'midpoint')\n",
        "  training_y_iqr = training_y_q3 - training_y_q1\n",
        "  training_y_low_lim = training_y_q1 - 1.5 * training_y_iqr\n",
        "  training_y_up_lim = training_y_q3 + 1.5 * training_y_iqr\n",
        "\n",
        "  training_data = []\n",
        "  for x, y in training_dataset:\n",
        "    if((eval(x) >= training_x_low_lim and eval(x) <= training_x_up_lim) and (eval(y) >= training_y_low_lim and eval(y) <= training_y_up_lim)):\n",
        "      training_data.append([eval(x), eval(y)])\n",
        "\n",
        "  validation_data = []\n",
        "  for x, y in validation_dataset:\n",
        "    if((eval(x) >= training_x_low_lim and eval(x) <= training_x_up_lim) and (eval(y) >= training_y_low_lim and eval(y) <= training_y_up_lim)):\n",
        "      validation_data.append([eval(x), eval(y)])\n",
        "\n",
        "  return np.array(training_data), np.array(validation_data)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data = PreprocessData(training_datalist, training_dataset, validation_dataset)"
      ],
      "metadata": {
        "id": "mqdNMAE7bIW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent(training_dataset, w, b, learning_rate):\n",
        "  dldw = 0.0\n",
        "  dldb = 0.0\n",
        "  N = len(training_dataset)\n",
        "  for x, y in training_dataset:\n",
        "    dldw += -2*eval(x)*(eval(y) - (w*eval(x)+b))\n",
        "    dldb += -2*(eval(y) - (w*eval(x)+b))\n",
        "\n",
        "  w = w - learning_rate*(1/N)*dldw\n",
        "  b = b - learning_rate*(1/N)*dldb\n",
        "\n",
        "  return w, b\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0000001\n",
        "w = 0.8\n",
        "b = 46\n",
        "x = [row[0] for row in training_data]\n",
        "y = [row[1] for row in training_data]\n",
        "loss_his = []\n",
        "w_his = []\n",
        "b_his = []"
      ],
      "metadata": {
        "id": "vpI45jS8zFwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(1000):\n",
        "  w, b = GradientDescent(training_dataset, w, b, learning_rate)\n",
        "  loss = 0\n",
        "  for i in range(len(x)):\n",
        "    yhat = w*x[i] + b\n",
        "    loss += (y[i]-yhat)**2\n",
        "  print(f'iteration {_+1} loss = {loss} w = {w} b = {b}')\n",
        "  loss_his.append(loss)\n",
        "  w_his.append(w)\n",
        "  b_his.append(b)\n",
        "  coefficient_output.append([b, w])"
      ],
      "metadata": {
        "id": "4XCxMiXVznYF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0ecf988-e511-4ae2-f8f3-78b2ff2be8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1 loss = 105420.4993748771 w = 0.8003550312857143 b = 46.00000413142857\n",
            "iteration 2 loss = 105133.445423654 w = 0.8007095679665471 b = 46.000008257003365\n",
            "iteration 3 loss = 104847.27555918993 w = 0.8010636107315492 b = 46.00001237673255\n",
            "iteration 4 loss = 104561.98720171233 w = 0.8014171602688113 b = 46.00001649062426\n",
            "iteration 5 loss = 104277.57777879624 w = 0.8017702172654655 b = 46.00002059868664\n",
            "iteration 6 loss = 103994.0447253437 w = 0.8021227824076869 b = 46.000024700927796\n",
            "iteration 7 loss = 103711.38548356299 w = 0.8024748563806943 b = 46.000028797355846\n",
            "iteration 8 loss = 103429.59750294812 w = 0.8028264398687521 b = 46.00003288797888\n",
            "iteration 9 loss = 103148.67824025803 w = 0.8031775335551716 b = 46.000036972805\n",
            "iteration 10 loss = 102868.62515949647 w = 0.8035281381223118 b = 46.00004105184227\n",
            "iteration 11 loss = 102589.435731892 w = 0.8038782542515813 b = 46.00004512509875\n",
            "iteration 12 loss = 102311.10743587643 w = 0.8042278826234392 b = 46.00004919258251\n",
            "iteration 13 loss = 102033.63775706575 w = 0.804577023917397 b = 46.00005325430158\n",
            "iteration 14 loss = 101757.02418823894 w = 0.8049256788120193 b = 46.00005731026399\n",
            "iteration 15 loss = 101481.26422931891 w = 0.8052738479849252 b = 46.000061360477766\n",
            "iteration 16 loss = 101206.3553873515 w = 0.8056215321127901 b = 46.00006540495092\n",
            "iteration 17 loss = 100932.29517648507 w = 0.8059687318713467 b = 46.00006944369144\n",
            "iteration 18 loss = 100659.081117952 w = 0.806315447935386 b = 46.000073476707314\n",
            "iteration 19 loss = 100386.7107400474 w = 0.8066616809787592 b = 46.00007750400652\n",
            "iteration 20 loss = 100115.1815781096 w = 0.8070074316743787 b = 46.00008152559702\n",
            "iteration 21 loss = 99844.4911745005 w = 0.8073527006942194 b = 46.00008554148677\n",
            "iteration 22 loss = 99574.63707858569 w = 0.80769748870932 b = 46.00008955168371\n",
            "iteration 23 loss = 99305.6168467144 w = 0.8080417963897845 b = 46.00009355619577\n",
            "iteration 24 loss = 99037.42804220052 w = 0.8083856244047831 b = 46.00009755503088\n",
            "iteration 25 loss = 98770.0682353025 w = 0.8087289734225541 b = 46.00010154819693\n",
            "iteration 26 loss = 98503.53500320364 w = 0.8090718441104046 b = 46.00010553570183\n",
            "iteration 27 loss = 98237.82592999318 w = 0.8094142371347121 b = 46.00010951755346\n",
            "iteration 28 loss = 97972.938606646 w = 0.8097561531609258 b = 46.000113493759706\n",
            "iteration 29 loss = 97708.87063100428 w = 0.8100975928535677 b = 46.000117464328426\n",
            "iteration 30 loss = 97445.61960775749 w = 0.8104385568762343 b = 46.00012142926747\n",
            "iteration 31 loss = 97183.18314842292 w = 0.8107790458915974 b = 46.00012538858469\n",
            "iteration 32 loss = 96921.55887132685 w = 0.8111190605614057 b = 46.00012934228791\n",
            "iteration 33 loss = 96660.74440158538 w = 0.811458601546486 b = 46.00013329038496\n",
            "iteration 34 loss = 96400.73737108523 w = 0.8117976695067444 b = 46.00013723288364\n",
            "iteration 35 loss = 96141.53541846432 w = 0.8121362651011677 b = 46.00014116979175\n",
            "iteration 36 loss = 95883.1361890933 w = 0.8124743889878245 b = 46.00014510111709\n",
            "iteration 37 loss = 95625.53733505645 w = 0.8128120418238671 b = 46.00014902686743\n",
            "iteration 38 loss = 95368.73651513254 w = 0.8131492242655318 b = 46.00015294705054\n",
            "iteration 39 loss = 95112.73139477582 w = 0.8134859369681409 b = 46.000156861674164\n",
            "iteration 40 loss = 94857.519646098 w = 0.8138221805861037 b = 46.00016077074606\n",
            "iteration 41 loss = 94603.09894784863 w = 0.8141579557729177 b = 46.000164674273954\n",
            "iteration 42 loss = 94349.46698539674 w = 0.8144932631811703 b = 46.00016857226558\n",
            "iteration 43 loss = 94096.62145071228 w = 0.8148281034625395 b = 46.00017246472864\n",
            "iteration 44 loss = 93844.56004234763 w = 0.8151624772677954 b = 46.000176351670845\n",
            "iteration 45 loss = 93593.2804654186 w = 0.8154963852468018 b = 46.000180233099876\n",
            "iteration 46 loss = 93342.78043158645 w = 0.8158298280485167 b = 46.00018410902342\n",
            "iteration 47 loss = 93093.05765903875 w = 0.8161628063209944 b = 46.00018797944915\n",
            "iteration 48 loss = 92844.10987247211 w = 0.8164953207113861 b = 46.00019184438472\n",
            "iteration 49 loss = 92595.93480307273 w = 0.8168273718659417 b = 46.00019570383778\n",
            "iteration 50 loss = 92348.53018849861 w = 0.8171589604300106 b = 46.00019955781597\n",
            "iteration 51 loss = 92101.89377286151 w = 0.8174900870480432 b = 46.00020340632691\n",
            "iteration 52 loss = 91856.02330670806 w = 0.817820752363592 b = 46.00020724937822\n",
            "iteration 53 loss = 91610.91654700274 w = 0.8181509570193131 b = 46.00021108697751\n",
            "iteration 54 loss = 91366.57125710847 w = 0.8184807016569672 b = 46.00021491913237\n",
            "iteration 55 loss = 91122.9852067698 w = 0.818809986917421 b = 46.00021874585039\n",
            "iteration 56 loss = 90880.15617209415 w = 0.8191388134406483 b = 46.00022256713914\n",
            "iteration 57 loss = 90638.08193553398 w = 0.8194671818657315 b = 46.000226383006186\n",
            "iteration 58 loss = 90396.76028586936 w = 0.8197950928308625 b = 46.00023019345908\n",
            "iteration 59 loss = 90156.18901818979 w = 0.8201225469733441 b = 46.00023399850537\n",
            "iteration 60 loss = 89916.36593387625 w = 0.8204495449295913 b = 46.00023779815258\n",
            "iteration 61 loss = 89677.28884058371 w = 0.8207760873351326 b = 46.00024159240824\n",
            "iteration 62 loss = 89438.95555222358 w = 0.821102174824611 b = 46.00024538127985\n",
            "iteration 63 loss = 89201.3638889458 w = 0.8214278080317854 b = 46.00024916477492\n",
            "iteration 64 loss = 88964.51167712126 w = 0.8217529875895317 b = 46.000252942900936\n",
            "iteration 65 loss = 88728.39674932492 w = 0.8220777141298443 b = 46.00025671566538\n",
            "iteration 66 loss = 88493.01694431681 w = 0.822401988283837 b = 46.00026048307572\n",
            "iteration 67 loss = 88258.37010702648 w = 0.8227258106817446 b = 46.00026424513942\n",
            "iteration 68 loss = 88024.45408853439 w = 0.8230491819529234 b = 46.00026800186392\n",
            "iteration 69 loss = 87791.26674605478 w = 0.8233721027258535 b = 46.00027175325666\n",
            "iteration 70 loss = 87558.8059429185 w = 0.823694573628139 b = 46.00027549932508\n",
            "iteration 71 loss = 87327.06954855596 w = 0.82401659528651 b = 46.00027924007658\n",
            "iteration 72 loss = 87096.0554384797 w = 0.8243381683268232 b = 46.00028297551857\n",
            "iteration 73 loss = 86865.76149426706 w = 0.8246592933740635 b = 46.00028670565846\n",
            "iteration 74 loss = 86636.18560354337 w = 0.8249799710523453 b = 46.000290430503625\n",
            "iteration 75 loss = 86407.32565996502 w = 0.8253002019849134 b = 46.00029415006144\n",
            "iteration 76 loss = 86179.17956320227 w = 0.8256199867941441 b = 46.00029786433928\n",
            "iteration 77 loss = 85951.74521892211 w = 0.8259393261015469 b = 46.000301573344494\n",
            "iteration 78 loss = 85725.0205387718 w = 0.8262582205277654 b = 46.00030527708443\n",
            "iteration 79 loss = 85499.0034403614 w = 0.8265766706925787 b = 46.000308975566426\n",
            "iteration 80 loss = 85273.69184724768 w = 0.8268946772149022 b = 46.000312668797804\n",
            "iteration 81 loss = 85049.0836889169 w = 0.8272122407127892 b = 46.00031635678588\n",
            "iteration 82 loss = 84825.17690076813 w = 0.8275293618034321 b = 46.000320039537954\n",
            "iteration 83 loss = 84601.96942409666 w = 0.8278460411031632 b = 46.00032371706133\n",
            "iteration 84 loss = 84379.45920607747 w = 0.8281622792274563 b = 46.00032738936328\n",
            "iteration 85 loss = 84157.6441997486 w = 0.8284780767909279 b = 46.00033105645108\n",
            "iteration 86 loss = 83936.5223639943 w = 0.8287934344073379 b = 46.000334718332006\n",
            "iteration 87 loss = 83716.09166352895 w = 0.8291083526895915 b = 46.0003383750133\n",
            "iteration 88 loss = 83496.35006888055 w = 0.8294228322497399 b = 46.00034202650221\n",
            "iteration 89 loss = 83277.29555637424 w = 0.8297368736989816 b = 46.00034567280597\n",
            "iteration 90 loss = 83058.92610811572 w = 0.8300504776476638 b = 46.00034931393181\n",
            "iteration 91 loss = 82841.23971197588 w = 0.8303636447052831 b = 46.000352949886924\n",
            "iteration 92 loss = 82624.23436157312 w = 0.8306763754804872 b = 46.000356580678535\n",
            "iteration 93 loss = 82407.90805625837 w = 0.830988670581076 b = 46.00036020631383\n",
            "iteration 94 loss = 82192.25880109819 w = 0.8313005306140022 b = 46.00036382679999\n",
            "iteration 95 loss = 81977.28460685926 w = 0.8316119561853734 b = 46.00036744214419\n",
            "iteration 96 loss = 81762.98348999149 w = 0.8319229479004526 b = 46.00037105235359\n",
            "iteration 97 loss = 81549.35347261302 w = 0.8322335063636596 b = 46.00037465743536\n",
            "iteration 98 loss = 81336.39258249332 w = 0.8325436321785721 b = 46.00037825739662\n",
            "iteration 99 loss = 81124.09885303756 w = 0.832853325947927 b = 46.000381852244516\n",
            "iteration 100 loss = 80912.47032327096 w = 0.8331625882736218 b = 46.000385441986175\n",
            "iteration 101 loss = 80701.50503782276 w = 0.8334714197567149 b = 46.0003890266287\n",
            "iteration 102 loss = 80491.2010469104 w = 0.8337798209974278 b = 46.000392606179204\n",
            "iteration 103 loss = 80281.55640632391 w = 0.8340877925951454 b = 46.00039618064478\n",
            "iteration 104 loss = 80072.56917740968 w = 0.8343953351484181 b = 46.00039975003251\n",
            "iteration 105 loss = 79864.23742705533 w = 0.8347024492549622 b = 46.00040331434946\n",
            "iteration 106 loss = 79656.55922767412 w = 0.835009135511661 b = 46.00040687360271\n",
            "iteration 107 loss = 79449.53265718902 w = 0.8353153945145667 b = 46.000410427799295\n",
            "iteration 108 loss = 79243.15579901733 w = 0.8356212268589011 b = 46.00041397694628\n",
            "iteration 109 loss = 79037.42674205493 w = 0.8359266331390564 b = 46.000417521050686\n",
            "iteration 110 loss = 78832.34358066166 w = 0.8362316139485969 b = 46.00042106011954\n",
            "iteration 111 loss = 78627.9044146447 w = 0.8365361698802602 b = 46.00042459415987\n",
            "iteration 112 loss = 78424.1073492442 w = 0.836840301525958 b = 46.000428123178665\n",
            "iteration 113 loss = 78220.95049511733 w = 0.8371440094767774 b = 46.00043164718293\n",
            "iteration 114 loss = 78018.43196832346 w = 0.8374472943229818 b = 46.00043516617964\n",
            "iteration 115 loss = 77816.54989030852 w = 0.8377501566540126 b = 46.00043868017578\n",
            "iteration 116 loss = 77615.30238789016 w = 0.83805259705849 b = 46.00044218917832\n",
            "iteration 117 loss = 77414.68759324227 w = 0.838354616124214 b = 46.000445693194216\n",
            "iteration 118 loss = 77214.70364388022 w = 0.8386562144381658 b = 46.000449192230406\n",
            "iteration 119 loss = 77015.34868264536 w = 0.838957392586509 b = 46.00045268629383\n",
            "iteration 120 loss = 76816.62085769049 w = 0.8392581511545903 b = 46.00045617539143\n",
            "iteration 121 loss = 76618.51832246443 w = 0.8395584907269412 b = 46.0004596595301\n",
            "iteration 122 loss = 76421.03923569722 w = 0.8398584118872787 b = 46.00046313871677\n",
            "iteration 123 loss = 76224.1817613855 w = 0.8401579152185068 b = 46.00046661295832\n",
            "iteration 124 loss = 76027.94406877742 w = 0.8404570013027172 b = 46.00047008226165\n",
            "iteration 125 loss = 75832.32433235752 w = 0.8407556707211907 b = 46.00047354663364\n",
            "iteration 126 loss = 75637.32073183254 w = 0.8410539240543985 b = 46.00047700608116\n",
            "iteration 127 loss = 75442.9314521163 w = 0.8413517618820029 b = 46.00048046061107\n",
            "iteration 128 loss = 75249.15468331521 w = 0.8416491847828589 b = 46.00048391023022\n",
            "iteration 129 loss = 75055.98862071351 w = 0.8419461933350147 b = 46.00048735494545\n",
            "iteration 130 loss = 74863.43146475843 w = 0.8422427881157137 b = 46.00049079476359\n",
            "iteration 131 loss = 74671.48142104608 w = 0.8425389697013947 b = 46.00049422969147\n",
            "iteration 132 loss = 74480.1367003069 w = 0.8428347386676937 b = 46.00049765973589\n",
            "iteration 133 loss = 74289.39551839062 w = 0.8431300955894444 b = 46.00050108490367\n",
            "iteration 134 loss = 74099.25609625241 w = 0.8434250410406802 b = 46.00050450520159\n",
            "iteration 135 loss = 73909.71665993806 w = 0.8437195755946344 b = 46.000507920636444\n",
            "iteration 136 loss = 73720.77544056991 w = 0.844013699823742 b = 46.000511331215\n",
            "iteration 137 loss = 73532.43067433238 w = 0.8443074142996402 b = 46.00051473694403\n",
            "iteration 138 loss = 73344.6806024579 w = 0.8446007195931701 b = 46.00051813783028\n",
            "iteration 139 loss = 73157.52347121204 w = 0.8448936162743773 b = 46.0005215338805\n",
            "iteration 140 loss = 72970.95753188006 w = 0.8451861049125136 b = 46.00052492510144\n",
            "iteration 141 loss = 72784.98104075257 w = 0.8454781860760373 b = 46.00052831149981\n",
            "iteration 142 loss = 72599.59225911077 w = 0.8457698603326153 b = 46.00053169308234\n",
            "iteration 143 loss = 72414.78945321345 w = 0.846061128249123 b = 46.00053506985573\n",
            "iteration 144 loss = 72230.57089428173 w = 0.8463519903916469 b = 46.000538441826684\n",
            "iteration 145 loss = 72046.93485848617 w = 0.846642447325484 b = 46.000541809001895\n",
            "iteration 146 loss = 71863.87962693194 w = 0.8469324996151444 b = 46.00054517138805\n",
            "iteration 147 loss = 71681.40348564551 w = 0.8472221478243515 b = 46.00054852899181\n",
            "iteration 148 loss = 71499.50472556053 w = 0.8475113925160432 b = 46.00055188181984\n",
            "iteration 149 loss = 71318.18164250364 w = 0.8478002342523736 b = 46.00055522987879\n",
            "iteration 150 loss = 71137.4325371812 w = 0.8480886735947133 b = 46.00055857317531\n",
            "iteration 151 loss = 70957.25571516542 w = 0.8483767111036509 b = 46.000561911716034\n",
            "iteration 152 loss = 70777.64948688015 w = 0.848664347338994 b = 46.00056524550759\n",
            "iteration 153 loss = 70598.61216758768 w = 0.8489515828597705 b = 46.00056857455658\n",
            "iteration 154 loss = 70420.14207737507 w = 0.8492384182242292 b = 46.00057189886963\n",
            "iteration 155 loss = 70242.23754114012 w = 0.8495248539898415 b = 46.00057521845333\n",
            "iteration 156 loss = 70064.89688857824 w = 0.8498108907133018 b = 46.00057853331426\n",
            "iteration 157 loss = 69888.11845416865 w = 0.8500965289505295 b = 46.00058184345902\n",
            "iteration 158 loss = 69711.90057716104 w = 0.8503817692566689 b = 46.00058514889415\n",
            "iteration 159 loss = 69536.24160156192 w = 0.8506666121860912 b = 46.00058844962624\n",
            "iteration 160 loss = 69361.13987612119 w = 0.8509510582923955 b = 46.00059174566183\n",
            "iteration 161 loss = 69186.59375431915 w = 0.8512351081284093 b = 46.000595037007464\n",
            "iteration 162 loss = 69012.60159435235 w = 0.8515187622461902 b = 46.00059832366968\n",
            "iteration 163 loss = 68839.16175912126 w = 0.8518020211970264 b = 46.00060160565499\n",
            "iteration 164 loss = 68666.27261621616 w = 0.8520848855314385 b = 46.000604882969924\n",
            "iteration 165 loss = 68493.9325379043 w = 0.8523673557991798 b = 46.00060815562098\n",
            "iteration 166 loss = 68322.13990111668 w = 0.8526494325492379 b = 46.00061142361466\n",
            "iteration 167 loss = 68150.89308743482 w = 0.8529311163298354 b = 46.00061468695744\n",
            "iteration 168 loss = 67980.19048307749 w = 0.8532124076884315 b = 46.00061794565581\n",
            "iteration 169 loss = 67810.03047888796 w = 0.8534933071717224 b = 46.00062119971624\n",
            "iteration 170 loss = 67640.41147032051 w = 0.8537738153256429 b = 46.00062444914519\n",
            "iteration 171 loss = 67471.3318574276 w = 0.854053932695367 b = 46.00062769394912\n",
            "iteration 172 loss = 67302.79004484734 w = 0.8543336598253094 b = 46.00063093413446\n",
            "iteration 173 loss = 67134.78444178961 w = 0.8546129972591262 b = 46.00063416970766\n",
            "iteration 174 loss = 66967.31346202393 w = 0.8548919455397164 b = 46.000637400675124\n",
            "iteration 175 loss = 66800.37552386643 w = 0.8551705052092222 b = 46.00064062704328\n",
            "iteration 176 loss = 66633.96905016656 w = 0.855448676809031 b = 46.00064384881854\n",
            "iteration 177 loss = 66468.09246829475 w = 0.8557264608797757 b = 46.000647066007296\n",
            "iteration 178 loss = 66302.74421012966 w = 0.8560038579613359 b = 46.00065027861594\n",
            "iteration 179 loss = 66137.92271204513 w = 0.8562808685928396 b = 46.000653486650855\n",
            "iteration 180 loss = 65973.6264148976 w = 0.8565574933126632 b = 46.00065669011841\n",
            "iteration 181 loss = 65809.85376401374 w = 0.8568337326584333 b = 46.000659889024966\n",
            "iteration 182 loss = 65646.60320917741 w = 0.8571095871670275 b = 46.00066308337688\n",
            "iteration 183 loss = 65483.87320461709 w = 0.8573850573745754 b = 46.0006662731805\n",
            "iteration 184 loss = 65321.66220899397 w = 0.8576601438164597 b = 46.000669458442154\n",
            "iteration 185 loss = 65159.96868538837 w = 0.8579348470273174 b = 46.00067263916818\n",
            "iteration 186 loss = 64998.79110128819 w = 0.8582091675410404 b = 46.000675815364886\n",
            "iteration 187 loss = 64838.12792857606 w = 0.858483105890777 b = 46.00067898703859\n",
            "iteration 188 loss = 64677.977643516875 w = 0.8587566626089328 b = 46.00068215419559\n",
            "iteration 189 loss = 64518.338726745504 w = 0.8590298382271715 b = 46.00068531684217\n",
            "iteration 190 loss = 64359.20966325437 w = 0.8593026332764163 b = 46.000688474984635\n",
            "iteration 191 loss = 64200.58894238144 w = 0.8595750482868507 b = 46.00069162862924\n",
            "iteration 192 loss = 64042.47505779754 w = 0.8598470837879195 b = 46.000694777782265\n",
            "iteration 193 loss = 63884.866507494306 w = 0.8601187403083301 b = 46.00069792244996\n",
            "iteration 194 loss = 63727.76179377204 w = 0.8603900183760531 b = 46.00070106263857\n",
            "iteration 195 loss = 63571.15942322748 w = 0.8606609185183237 b = 46.00070419835434\n",
            "iteration 196 loss = 63415.057906741495 w = 0.8609314412616427 b = 46.0007073296035\n",
            "iteration 197 loss = 63259.45575946733 w = 0.8612015871317771 b = 46.00071045639228\n",
            "iteration 198 loss = 63104.351500818164 w = 0.8614713566537617 b = 46.00071357872688\n",
            "iteration 199 loss = 62949.743654455386 w = 0.8617407503518999 b = 46.000716696613516\n",
            "iteration 200 loss = 62795.63074827636 w = 0.8620097687497644 b = 46.00071981005838\n",
            "iteration 201 loss = 62642.0113144025 w = 0.8622784123701989 b = 46.000722919067655\n",
            "iteration 202 loss = 62488.8838891676 w = 0.8625466817353181 b = 46.00072602364753\n",
            "iteration 203 loss = 62336.24701310539 w = 0.86281457736651 b = 46.00072912380417\n",
            "iteration 204 loss = 62184.099230938264 w = 0.8630820997844358 b = 46.00073221954374\n",
            "iteration 205 loss = 62032.43909156512 w = 0.8633492495090315 b = 46.00073531087239\n",
            "iteration 206 loss = 61881.26514804944 w = 0.8636160270595086 b = 46.000738397796276\n",
            "iteration 207 loss = 61730.57595760798 w = 0.8638824329543555 b = 46.00074148032152\n",
            "iteration 208 loss = 61580.37008159836 w = 0.8641484677113382 b = 46.00074455845426\n",
            "iteration 209 loss = 61430.64608550815 w = 0.8644141318475014 b = 46.00074763220061\n",
            "iteration 210 loss = 61281.40253894244 w = 0.8646794258791693 b = 46.00075070156668\n",
            "iteration 211 loss = 61132.638015612865 w = 0.8649443503219472 b = 46.000753766558574\n",
            "iteration 212 loss = 60984.351093325386 w = 0.8652089056907215 b = 46.000756827182386\n",
            "iteration 213 loss = 60836.54035396917 w = 0.8654730924996619 b = 46.0007598834442\n",
            "iteration 214 loss = 60689.20438350507 w = 0.8657369112622213 b = 46.0007629353501\n",
            "iteration 215 loss = 60542.341771953594 w = 0.8660003624911377 b = 46.00076598290615\n",
            "iteration 216 loss = 60395.95111338407 w = 0.8662634466984345 b = 46.000769026118405\n",
            "iteration 217 loss = 60250.0310059027 w = 0.8665261643954218 b = 46.00077206499292\n",
            "iteration 218 loss = 60104.58005164157 w = 0.8667885160926977 b = 46.000775099535744\n",
            "iteration 219 loss = 59959.596856747106 w = 0.8670505023001485 b = 46.0007781297529\n",
            "iteration 220 loss = 59815.08003136833 w = 0.8673121235269504 b = 46.00078115565042\n",
            "iteration 221 loss = 59671.02818964619 w = 0.8675733802815704 b = 46.00078417723432\n",
            "iteration 222 loss = 59527.43994970191 w = 0.8678342730717669 b = 46.000787194510615\n",
            "iteration 223 loss = 59384.31393362581 w = 0.8680948024045909 b = 46.0007902074853\n",
            "iteration 224 loss = 59241.6487674662 w = 0.8683549687863872 b = 46.000793216164375\n",
            "iteration 225 loss = 59099.443081217854 w = 0.8686147727227951 b = 46.000796220553816\n",
            "iteration 226 loss = 58957.69550881129 w = 0.8688742147187494 b = 46.0007992206596\n",
            "iteration 227 loss = 58816.40468810135 w = 0.8691332952784816 b = 46.000802216487706\n",
            "iteration 228 loss = 58675.569260855955 w = 0.8693920149055208 b = 46.00080520804408\n",
            "iteration 229 loss = 58535.187872745846 w = 0.8696503741026942 b = 46.00080819533468\n",
            "iteration 230 loss = 58395.25917333251 w = 0.869908373372129 b = 46.000811178365446\n",
            "iteration 231 loss = 58255.781816057744 w = 0.8701660132152527 b = 46.000814157142315\n",
            "iteration 232 loss = 58116.75445823257 w = 0.8704232941327941 b = 46.00081713167121\n",
            "iteration 233 loss = 57978.17576102646 w = 0.8706802166247845 b = 46.00082010195805\n",
            "iteration 234 loss = 57840.044389456125 w = 0.8709367811905586 b = 46.00082306800874\n",
            "iteration 235 loss = 57702.35901237487 w = 0.8711929883287556 b = 46.00082602982919\n",
            "iteration 236 loss = 57565.11830246166 w = 0.8714488385373197 b = 46.000828987425294\n",
            "iteration 237 loss = 57428.32093621016 w = 0.8717043323135018 b = 46.00083194080293\n",
            "iteration 238 loss = 57291.9655939183 w = 0.8719594701538597 b = 46.00083488996798\n",
            "iteration 239 loss = 57156.05095967725 w = 0.8722142525542596 b = 46.00083783492631\n",
            "iteration 240 loss = 57020.57572136065 w = 0.8724686800098769 b = 46.00084077568378\n",
            "iteration 241 loss = 56885.538570614066 w = 0.8727227530151972 b = 46.00084371224624\n",
            "iteration 242 loss = 56750.93820284444 w = 0.8729764720640171 b = 46.000846644619536\n",
            "iteration 243 loss = 56616.77331720895 w = 0.8732298376494453 b = 46.00084957280951\n",
            "iteration 244 loss = 56483.042616605075 w = 0.8734828502639036 b = 46.000852496821985\n",
            "iteration 245 loss = 56349.74480765942 w = 0.8737355103991278 b = 46.00085541666278\n",
            "iteration 246 loss = 56216.878600717595 w = 0.8739878185461685 b = 46.000858332337714\n",
            "iteration 247 loss = 56084.44270983348 w = 0.8742397751953923 b = 46.00086124385258\n",
            "iteration 248 loss = 55952.43585275875 w = 0.8744913808364827 b = 46.000864151213186\n",
            "iteration 249 loss = 55820.85675093255 w = 0.8747426359584409 b = 46.000867054425306\n",
            "iteration 250 loss = 55689.704129470905 w = 0.8749935410495869 b = 46.00086995349473\n",
            "iteration 251 loss = 55558.9767171562 w = 0.8752440965975605 b = 46.00087284842722\n",
            "iteration 252 loss = 55428.673246427476 w = 0.875494303089322 b = 46.00087573922855\n",
            "iteration 253 loss = 55298.79245336906 w = 0.8757441610111533 b = 46.00087862590446\n",
            "iteration 254 loss = 55169.333077701085 w = 0.875993670848659 b = 46.000881508460715\n",
            "iteration 255 loss = 55040.293862768856 w = 0.8762428330867671 b = 46.00088438690304\n",
            "iteration 256 loss = 54911.67355553266 w = 0.87649164820973 b = 46.00088726123717\n",
            "iteration 257 loss = 54783.47090655738 w = 0.8767401167011256 b = 46.00089013146883\n",
            "iteration 258 loss = 54655.68467000271 w = 0.8769882390438579 b = 46.000892997603735\n",
            "iteration 259 loss = 54528.31360361264 w = 0.8772360157201583 b = 46.00089585964759\n",
            "iteration 260 loss = 54401.35646870524 w = 0.8774834472115863 b = 46.000898717606105\n",
            "iteration 261 loss = 54274.81203016276 w = 0.8777305339990307 b = 46.00090157148496\n",
            "iteration 262 loss = 54148.6790564217 w = 0.8779772765627101 b = 46.00090442128984\n",
            "iteration 263 loss = 54022.956319462304 w = 0.8782236753821743 b = 46.00090726702642\n",
            "iteration 264 loss = 53897.64259479878 w = 0.8784697309363049 b = 46.000910108700374\n",
            "iteration 265 loss = 53772.73666146959 w = 0.8787154437033163 b = 46.00091294631736\n",
            "iteration 266 loss = 53648.237302026704 w = 0.878960814160757 b = 46.00091577988302\n",
            "iteration 267 loss = 53524.14330252657 w = 0.8792058427855098 b = 46.000918609403016\n",
            "iteration 268 loss = 53400.453452519556 w = 0.8794505300537935 b = 46.00092143488297\n",
            "iteration 269 loss = 53277.16654504012 w = 0.8796948764411632 b = 46.00092425632852\n",
            "iteration 270 loss = 53154.28137659751 w = 0.8799388824225116 b = 46.000927073745274\n",
            "iteration 271 loss = 53031.796747165135 w = 0.8801825484720698 b = 46.00092988713885\n",
            "iteration 272 loss = 52909.71146017135 w = 0.8804258750634082 b = 46.00093269651486\n",
            "iteration 273 loss = 52788.02432248942 w = 0.8806688626694374 b = 46.00093550187889\n",
            "iteration 274 loss = 52666.734144427726 w = 0.8809115117624096 b = 46.000938303236545\n",
            "iteration 275 loss = 52545.83973972045 w = 0.8811538228139183 b = 46.00094110059339\n",
            "iteration 276 loss = 52425.33992551707 w = 0.8813957962949007 b = 46.000943893955004\n",
            "iteration 277 loss = 52305.233522373666 w = 0.8816374326756377 b = 46.00094668332695\n",
            "iteration 278 loss = 52185.51935424269 w = 0.8818787324257548 b = 46.00094946871479\n",
            "iteration 279 loss = 52066.19624846329 w = 0.8821196960142236 b = 46.00095225012408\n",
            "iteration 280 loss = 51947.26303575195 w = 0.8823603239093621 b = 46.00095502756036\n",
            "iteration 281 loss = 51828.718550193036 w = 0.8826006165788363 b = 46.00095780102916\n",
            "iteration 282 loss = 51710.561629228934 w = 0.88284057448966 b = 46.00096057053601\n",
            "iteration 283 loss = 51592.79111365095 w = 0.8830801981081969 b = 46.00096333608642\n",
            "iteration 284 loss = 51475.40584758937 w = 0.8833194879001609 b = 46.00096609768592\n",
            "iteration 285 loss = 51358.404678504325 w = 0.8835584443306169 b = 46.00096885534\n",
            "iteration 286 loss = 51241.786457176255 w = 0.8837970678639822 b = 46.00097160905416\n",
            "iteration 287 loss = 51125.550037696434 w = 0.8840353589640269 b = 46.00097435883389\n",
            "iteration 288 loss = 51009.69427745807 w = 0.884273318093875 b = 46.00097710468467\n",
            "iteration 289 loss = 50894.21803714611 w = 0.8845109457160054 b = 46.000979846611976\n",
            "iteration 290 loss = 50779.12018072865 w = 0.8847482422922527 b = 46.000982584621276\n",
            "iteration 291 loss = 50664.399575447256 w = 0.884985208283808 b = 46.00098531871802\n",
            "iteration 292 loss = 50550.055091808026 w = 0.88522184415122 b = 46.000988048907665\n",
            "iteration 293 loss = 50436.08560357192 w = 0.8854581503543958 b = 46.00099077519565\n",
            "iteration 294 loss = 50322.48998774577 w = 0.8856941273526017 b = 46.00099349758742\n",
            "iteration 295 loss = 50209.26712457323 w = 0.8859297756044643 b = 46.000996216088396\n",
            "iteration 296 loss = 50096.415897525454 w = 0.8861650955679711 b = 46.00099893070399\n",
            "iteration 297 loss = 49983.93519329181 w = 0.8864000877004717 b = 46.00100164143963\n",
            "iteration 298 loss = 49871.82390177104 w = 0.8866347524586786 b = 46.00100434830071\n",
            "iteration 299 loss = 49760.08091606206 w = 0.8868690902986679 b = 46.00100705129264\n",
            "iteration 300 loss = 49648.70513245478 w = 0.8871031016758804 b = 46.0010097504208\n",
            "iteration 301 loss = 49537.69545042131 w = 0.8873367870451223 b = 46.00101244569057\n",
            "iteration 302 loss = 49427.050772606744 w = 0.8875701468605665 b = 46.00101513710734\n",
            "iteration 303 loss = 49316.77000482036 w = 0.8878031815757527 b = 46.00101782467646\n",
            "iteration 304 loss = 49206.85205602631 w = 0.8880358916435891 b = 46.0010205084033\n",
            "iteration 305 loss = 49097.29583833528 w = 0.888268277516353 b = 46.00102318829321\n",
            "iteration 306 loss = 48988.10026699471 w = 0.8885003396456913 b = 46.00102586435154\n",
            "iteration 307 loss = 48879.26426038086 w = 0.888732078482622 b = 46.001028536583625\n",
            "iteration 308 loss = 48770.786739989395 w = 0.8889634944775346 b = 46.001031204994796\n",
            "iteration 309 loss = 48662.666630426604 w = 0.8891945880801911 b = 46.00103386959037\n",
            "iteration 310 loss = 48554.90285940065 w = 0.8894253597397271 b = 46.00103653037568\n",
            "iteration 311 loss = 48447.49435771277 w = 0.8896558099046524 b = 46.001039187356014\n",
            "iteration 312 loss = 48340.44005924845 w = 0.8898859390228521 b = 46.00104184053668\n",
            "iteration 313 loss = 48233.738900969016 w = 0.890115747541587 b = 46.001044489922975\n",
            "iteration 314 loss = 48127.389822902354 w = 0.890345235907495 b = 46.00104713552018\n",
            "iteration 315 loss = 48021.391768134796 w = 0.890574404566592 b = 46.00104977733358\n",
            "iteration 316 loss = 47915.74368280204 w = 0.8908032539642721 b = 46.001052415368434\n",
            "iteration 317 loss = 47810.444516080584 w = 0.8910317845453092 b = 46.00105504963002\n",
            "iteration 318 loss = 47705.49322017945 w = 0.8912599967538576 b = 46.00105768012359\n",
            "iteration 319 loss = 47600.88875033097 w = 0.8914878910334526 b = 46.00106030685439\n",
            "iteration 320 loss = 47496.630064783 w = 0.8917154678270118 b = 46.00106292982767\n",
            "iteration 321 loss = 47392.71612478965 w = 0.8919427275768358 b = 46.001065549048654\n",
            "iteration 322 loss = 47289.14589460313 w = 0.8921696707246087 b = 46.00106816452258\n",
            "iteration 323 loss = 47185.91834146529 w = 0.8923962977113997 b = 46.00107077625466\n",
            "iteration 324 loss = 47083.03243559899 w = 0.8926226089776632 b = 46.001073384250105\n",
            "iteration 325 loss = 46980.487150199675 w = 0.8928486049632401 b = 46.00107598851413\n",
            "iteration 326 loss = 46878.28146142715 w = 0.8930742861073586 b = 46.00107858905193\n",
            "iteration 327 loss = 46776.414348396785 w = 0.8932996528486349 b = 46.00108118586869\n",
            "iteration 328 loss = 46674.88479317171 w = 0.8935247056250741 b = 46.00108377896961\n",
            "iteration 329 loss = 46573.69178075384 w = 0.8937494448740713 b = 46.00108636835985\n",
            "iteration 330 loss = 46472.83429907601 w = 0.8939738710324121 b = 46.00108895404458\n",
            "iteration 331 loss = 46372.311338993364 w = 0.8941979845362736 b = 46.00109153602897\n",
            "iteration 332 loss = 46272.121894275275 w = 0.8944217858212254 b = 46.00109411431818\n",
            "iteration 333 loss = 46172.264961597 w = 0.8946452753222301 b = 46.00109668891734\n",
            "iteration 334 loss = 46072.73954053148 w = 0.8948684534736445 b = 46.00109925983161\n",
            "iteration 335 loss = 45973.54463354124 w = 0.89509132070922 b = 46.001101827066115\n",
            "iteration 336 loss = 45874.67924596969 w = 0.8953138774621041 b = 46.001104390625976\n",
            "iteration 337 loss = 45776.14238603367 w = 0.8955361241648407 b = 46.001106950516316\n",
            "iteration 338 loss = 45677.93306481492 w = 0.8957580612493711 b = 46.00110950674225\n",
            "iteration 339 loss = 45580.05029625189 w = 0.8959796891470347 b = 46.00111205930889\n",
            "iteration 340 loss = 45482.493097131795 w = 0.8962010082885704 b = 46.001114608221314\n",
            "iteration 341 loss = 45385.26048708257 w = 0.8964220191041166 b = 46.00111715348463\n",
            "iteration 342 loss = 45288.351488564804 w = 0.8966427220232125 b = 46.00111969510391\n",
            "iteration 343 loss = 45191.76512686344 w = 0.8968631174747992 b = 46.00112223308424\n",
            "iteration 344 loss = 45095.50043008012 w = 0.89708320588722 b = 46.00112476743068\n",
            "iteration 345 loss = 44999.55642912507 w = 0.8973029876882216 b = 46.001127298148305\n",
            "iteration 346 loss = 44903.93215770921 w = 0.8975224633049546 b = 46.00112982524216\n",
            "iteration 347 loss = 44808.62665233591 w = 0.8977416331639745 b = 46.0011323487173\n",
            "iteration 348 loss = 44713.63895229352 w = 0.8979604976912429 b = 46.00113486857876\n",
            "iteration 349 loss = 44618.96809964714 w = 0.8981790573121275 b = 46.00113738483158\n",
            "iteration 350 loss = 44524.61313923086 w = 0.8983973124514039 b = 46.00113989748078\n",
            "iteration 351 loss = 44430.5731186399 w = 0.8986152635332555 b = 46.00114240653139\n",
            "iteration 352 loss = 44336.84708822279 w = 0.898832910981275 b = 46.001144911988426\n",
            "iteration 353 loss = 44243.434101073675 w = 0.899050255218465 b = 46.001147413856884\n",
            "iteration 354 loss = 44150.333213024256 w = 0.8992672966672387 b = 46.00114991214176\n",
            "iteration 355 loss = 44057.54348263626 w = 0.8994840357494207 b = 46.001152406848064\n",
            "iteration 356 loss = 43965.06397119361 w = 0.8997004728862482 b = 46.00115489798076\n",
            "iteration 357 loss = 43872.89374269469 w = 0.8999166084983714 b = 46.00115738554484\n",
            "iteration 358 loss = 43781.03186384473 w = 0.9001324430058546 b = 46.001159869545276\n",
            "iteration 359 loss = 43689.477404047924 w = 0.9003479768281767 b = 46.00116234998703\n",
            "iteration 360 loss = 43598.22943540029 w = 0.9005632103842324 b = 46.001164826875055\n",
            "iteration 361 loss = 43507.287032681175 w = 0.9007781440923327 b = 46.001167300214306\n",
            "iteration 362 loss = 43416.64927334643 w = 0.900992778370206 b = 46.001169770009724\n",
            "iteration 363 loss = 43326.315237520495 w = 0.9012071136349985 b = 46.00117223626625\n",
            "iteration 364 loss = 43236.28400798888 w = 0.9014211503032755 b = 46.00117469898881\n",
            "iteration 365 loss = 43146.554670190526 w = 0.9016348887910218 b = 46.00117715818234\n",
            "iteration 366 loss = 43057.126312210305 w = 0.9018483295136428 b = 46.00117961385174\n",
            "iteration 367 loss = 42967.99802477174 w = 0.902061472885965 b = 46.00118206600193\n",
            "iteration 368 loss = 42879.16890122917 w = 0.9022743193222373 b = 46.001184514637806\n",
            "iteration 369 loss = 42790.63803756054 w = 0.9024868692361312 b = 46.00118695976427\n",
            "iteration 370 loss = 42702.4045323599 w = 0.902699123040742 b = 46.001189401386206\n",
            "iteration 371 loss = 42614.46748682973 w = 0.9029110811485895 b = 46.0011918395085\n",
            "iteration 372 loss = 42526.826004774186 w = 0.903122743971619 b = 46.001194274136026\n",
            "iteration 373 loss = 42439.479192590865 w = 0.9033341119212014 b = 46.00119670527365\n",
            "iteration 374 loss = 42352.42615926417 w = 0.9035451854081351 b = 46.00119913292623\n",
            "iteration 375 loss = 42265.66601635756 w = 0.9037559648426459 b = 46.00120155709863\n",
            "iteration 376 loss = 42179.197878006285 w = 0.903966450634388 b = 46.0012039777957\n",
            "iteration 377 loss = 42093.02086091027 w = 0.9041766431924452 b = 46.00120639502227\n",
            "iteration 378 loss = 42007.13408432679 w = 0.904386542925331 b = 46.00120880878319\n",
            "iteration 379 loss = 41921.53667006289 w = 0.9045961502409904 b = 46.00121121908328\n",
            "iteration 380 loss = 41836.22774246871 w = 0.9048054655467993 b = 46.00121362592736\n",
            "iteration 381 loss = 41751.20642842973 w = 0.9050144892495666 b = 46.00121602932025\n",
            "iteration 382 loss = 41666.4718573598 w = 0.9052232217555344 b = 46.00121842926675\n",
            "iteration 383 loss = 41582.02316119442 w = 0.9054316634703786 b = 46.00122082577167\n",
            "iteration 384 loss = 41497.859474382436 w = 0.9056398147992103 b = 46.00122321883979\n",
            "iteration 385 loss = 41413.979933880066 w = 0.9058476761465759 b = 46.00122560847591\n",
            "iteration 386 loss = 41330.38367914326 w = 0.9060552479164584 b = 46.00122799468482\n",
            "iteration 387 loss = 41247.0698521206 w = 0.9062625305122779 b = 46.00123037747127\n",
            "iteration 388 loss = 41164.03759724629 w = 0.9064695243368925 b = 46.001232756840054\n",
            "iteration 389 loss = 41081.286061433166 w = 0.9066762297925993 b = 46.00123513279591\n",
            "iteration 390 loss = 40998.81439406563 w = 0.9068826472811345 b = 46.00123750534361\n",
            "iteration 391 loss = 40916.62174699246 w = 0.907088777203675 b = 46.001239874487894\n",
            "iteration 392 loss = 40834.70727452003 w = 0.9072946199608386 b = 46.00124224023351\n",
            "iteration 393 loss = 40753.07013340542 w = 0.9075001759526851 b = 46.00124460258518\n",
            "iteration 394 loss = 40671.70948284901 w = 0.9077054455787169 b = 46.00124696154764\n",
            "iteration 395 loss = 40590.62448448806 w = 0.9079104292378799 b = 46.00124931712562\n",
            "iteration 396 loss = 40509.814302389525 w = 0.9081151273285641 b = 46.00125166932382\n",
            "iteration 397 loss = 40429.27810304309 w = 0.9083195402486047 b = 46.00125401814696\n",
            "iteration 398 loss = 40349.0150553545 w = 0.9085236683952824 b = 46.00125636359974\n",
            "iteration 399 loss = 40269.02433063868 w = 0.9087275121653247 b = 46.001258705686844\n",
            "iteration 400 loss = 40189.30510261256 w = 0.9089310719549061 b = 46.00126104441297\n",
            "iteration 401 loss = 40109.85654738874 w = 0.9091343481596494 b = 46.001263379782806\n",
            "iteration 402 loss = 40030.67784346843 w = 0.9093373411746263 b = 46.00126571180102\n",
            "iteration 403 loss = 39951.76817173452 w = 0.9095400513943579 b = 46.00126804047228\n",
            "iteration 404 loss = 39873.12671544528 w = 0.9097424792128158 b = 46.001270365801254\n",
            "iteration 405 loss = 39794.752660227015 w = 0.9099446250234228 b = 46.00127268779259\n",
            "iteration 406 loss = 39716.6451940679 w = 0.9101464892190534 b = 46.00127500645095\n",
            "iteration 407 loss = 39638.80350731091 w = 0.910348072192035 b = 46.001277321780975\n",
            "iteration 408 loss = 39561.22679264729 w = 0.9105493743341483 b = 46.00127963378729\n",
            "iteration 409 loss = 39483.914245109794 w = 0.9107503960366282 b = 46.001281942474535\n",
            "iteration 410 loss = 39406.86506206609 w = 0.9109511376901648 b = 46.00128424784733\n",
            "iteration 411 loss = 39330.07844321223 w = 0.9111515996849034 b = 46.0012865499103\n",
            "iteration 412 loss = 39253.553590565614 w = 0.9113517824104465 b = 46.00128884866805\n",
            "iteration 413 loss = 39177.28970845926 w = 0.911551686255853 b = 46.001291144125176\n",
            "iteration 414 loss = 39101.28600353416 w = 0.9117513116096405 b = 46.00129343628629\n",
            "iteration 415 loss = 39025.541684733485 w = 0.9119506588597849 b = 46.00129572515598\n",
            "iteration 416 loss = 38950.055963295716 w = 0.9121497283937218 b = 46.00129801073883\n",
            "iteration 417 loss = 38874.8280527483 w = 0.912348520598347 b = 46.00130029303942\n",
            "iteration 418 loss = 38799.85716890102 w = 0.9125470358600174 b = 46.00130257206232\n",
            "iteration 419 loss = 38725.14252983951 w = 0.9127452745645515 b = 46.0013048478121\n",
            "iteration 420 loss = 38650.68335591884 w = 0.9129432370972302 b = 46.00130712029332\n",
            "iteration 421 loss = 38576.47886975711 w = 0.9131409238427981 b = 46.00130938951053\n",
            "iteration 422 loss = 38502.52829622902 w = 0.9133383351854634 b = 46.00131165546828\n",
            "iteration 423 loss = 38428.83086245915 w = 0.913535471508899 b = 46.00131391817111\n",
            "iteration 424 loss = 38355.38579781611 w = 0.9137323331962437 b = 46.00131617762355\n",
            "iteration 425 loss = 38282.19233390579 w = 0.9139289206301021 b = 46.001318433830136\n",
            "iteration 426 loss = 38209.24970456497 w = 0.9141252341925461 b = 46.00132068679539\n",
            "iteration 427 loss = 38136.55714585526 w = 0.9143212742651151 b = 46.00132293652382\n",
            "iteration 428 loss = 38064.11389605656 w = 0.914517041228817 b = 46.001325183019944\n",
            "iteration 429 loss = 37991.9191956608 w = 0.9147125354641292 b = 46.001327426288256\n",
            "iteration 430 loss = 37919.97228736572 w = 0.9149077573509986 b = 46.001329666333255\n",
            "iteration 431 loss = 37848.272416068554 w = 0.915102707268843 b = 46.00133190315944\n",
            "iteration 432 loss = 37776.81882885958 w = 0.9152973855965517 b = 46.001334136771284\n",
            "iteration 433 loss = 37705.61077501642 w = 0.915491792712486 b = 46.00133636717327\n",
            "iteration 434 loss = 37634.64750599723 w = 0.9156859289944803 b = 46.00133859436988\n",
            "iteration 435 loss = 37563.9282754349 w = 0.9158797948198423 b = 46.001340818365556\n",
            "iteration 436 loss = 37493.45233913061 w = 0.9160733905653543 b = 46.00134303916477\n",
            "iteration 437 loss = 37423.218955047916 w = 0.9162667166072737 b = 46.001345256771984\n",
            "iteration 438 loss = 37353.22738330637 w = 0.9164597733213335 b = 46.001347471191636\n",
            "iteration 439 loss = 37283.476886175646 w = 0.9166525610827436 b = 46.00134968242816\n",
            "iteration 440 loss = 37213.96672806916 w = 0.9168450802661909 b = 46.001351890486006\n",
            "iteration 441 loss = 37144.69617553798 w = 0.9170373312458405 b = 46.00135409536959\n",
            "iteration 442 loss = 37075.664497265214 w = 0.9172293143953361 b = 46.00135629708334\n",
            "iteration 443 loss = 37006.87096405931 w = 0.9174210300878007 b = 46.00135849563167\n",
            "iteration 444 loss = 36938.31484884838 w = 0.9176124786958381 b = 46.00136069101899\n",
            "iteration 445 loss = 36869.99542667424 w = 0.9178036605915322 b = 46.0013628832497\n",
            "iteration 446 loss = 36801.91197468594 w = 0.9179945761464492 b = 46.001365072328205\n",
            "iteration 447 loss = 36734.063772134425 w = 0.9181852257316373 b = 46.001367258258895\n",
            "iteration 448 loss = 36666.450100366084 w = 0.918375609717628 b = 46.00136944104615\n",
            "iteration 449 loss = 36599.070242816895 w = 0.9185657284744364 b = 46.00137162069436\n",
            "iteration 450 loss = 36531.923485006584 w = 0.9187555823715623 b = 46.00137379720788\n",
            "iteration 451 loss = 36465.00911453267 w = 0.9189451717779905 b = 46.00137597059109\n",
            "iteration 452 loss = 36398.32642106449 w = 0.919134497062192 b = 46.00137814084835\n",
            "iteration 453 loss = 36331.8746963372 w = 0.9193235585921243 b = 46.001380307984014\n",
            "iteration 454 loss = 36265.65323414633 w = 0.9195123567352325 b = 46.00138247200243\n",
            "iteration 455 loss = 36199.66133034143 w = 0.9197008918584495 b = 46.00138463290794\n",
            "iteration 456 loss = 36133.898282820424 w = 0.9198891643281972 b = 46.001386790704885\n",
            "iteration 457 loss = 36068.363391524035 w = 0.9200771745103871 b = 46.001388945397586\n",
            "iteration 458 loss = 36003.05595842939 w = 0.9202649227704207 b = 46.001391096990375\n",
            "iteration 459 loss = 35937.97528754485 w = 0.9204524094731906 b = 46.00139324548757\n",
            "iteration 460 loss = 35873.120684903806 w = 0.9206396349830812 b = 46.001395390893485\n",
            "iteration 461 loss = 35808.49145855909 w = 0.920826599663969 b = 46.001397533212426\n",
            "iteration 462 loss = 35744.086918577275 w = 0.9210133038792235 b = 46.00139967244869\n",
            "iteration 463 loss = 35679.90637703289 w = 0.9211997479917083 b = 46.00140180860657\n",
            "iteration 464 loss = 35615.94914800267 w = 0.9213859323637814 b = 46.00140394169036\n",
            "iteration 465 loss = 35552.21454755985 w = 0.9215718573572957 b = 46.001406071704345\n",
            "iteration 466 loss = 35488.70189376846 w = 0.9217575233336005 b = 46.0014081986528\n",
            "iteration 467 loss = 35425.41050667793 w = 0.9219429306535412 b = 46.00141032253999\n",
            "iteration 468 loss = 35362.339708317115 w = 0.9221280796774607 b = 46.001412443370185\n",
            "iteration 469 loss = 35299.48882268876 w = 0.9223129707651999 b = 46.00141456114764\n",
            "iteration 470 loss = 35236.85717576403 w = 0.9224976042760984 b = 46.00141667587661\n",
            "iteration 471 loss = 35174.444095476734 w = 0.9226819805689952 b = 46.00141878756134\n",
            "iteration 472 loss = 35112.248911717674 w = 0.9228661000022294 b = 46.00142089620608\n",
            "iteration 473 loss = 35050.27095632941 w = 0.9230499629336406 b = 46.00142300181505\n",
            "iteration 474 loss = 34988.509563100306 w = 0.9232335697205705 b = 46.00142510439249\n",
            "iteration 475 loss = 34926.964067759334 w = 0.9234169207198623 b = 46.00142720394262\n",
            "iteration 476 loss = 34865.63380797028 w = 0.9236000162878625 b = 46.00142930046966\n",
            "iteration 477 loss = 34804.51812332643 w = 0.923782856780421 b = 46.00143139397782\n",
            "iteration 478 loss = 34743.61635534504 w = 0.9239654425528921 b = 46.0014334844713\n",
            "iteration 479 loss = 34682.92784746172 w = 0.9241477739601349 b = 46.0014355719543\n",
            "iteration 480 loss = 34622.451945025176 w = 0.924329851356514 b = 46.00143765643103\n",
            "iteration 481 loss = 34562.187995291606 w = 0.9245116750959007 b = 46.00143973790566\n",
            "iteration 482 loss = 34502.13534741937 w = 0.924693245531673 b = 46.00144181638239\n",
            "iteration 483 loss = 34442.29335246356 w = 0.9248745630167167 b = 46.00144389186538\n",
            "iteration 484 loss = 34382.66136337061 w = 0.9250556279034259 b = 46.0014459643588\n",
            "iteration 485 loss = 34323.23873497262 w = 0.925236440543704 b = 46.00144803386683\n",
            "iteration 486 loss = 34264.02482398274 w = 0.925417001288964 b = 46.00145010039362\n",
            "iteration 487 loss = 34205.018988988886 w = 0.9255973104901292 b = 46.00145216394332\n",
            "iteration 488 loss = 34146.22059044913 w = 0.9257773684976341 b = 46.00145422452008\n",
            "iteration 489 loss = 34087.628990685815 w = 0.9259571756614253 b = 46.001456282128046\n",
            "iteration 490 loss = 34029.24355388081 w = 0.9261367323309615 b = 46.00145833677135\n",
            "iteration 491 loss = 33971.06364606976 w = 0.9263160388552146 b = 46.00146038845413\n",
            "iteration 492 loss = 33913.088635137 w = 0.9264950955826706 b = 46.00146243718049\n",
            "iteration 493 loss = 33855.31789081034 w = 0.9266739028613298 b = 46.001464482954574\n",
            "iteration 494 loss = 33797.75078465563 w = 0.9268524610387076 b = 46.00146652578048\n",
            "iteration 495 loss = 33740.38669007176 w = 0.9270307704618354 b = 46.001468565662314\n",
            "iteration 496 loss = 33683.2249822852 w = 0.9272088314772612 b = 46.001470602604186\n",
            "iteration 497 loss = 33626.26503834504 w = 0.9273866444310501 b = 46.00147263661019\n",
            "iteration 498 loss = 33569.506237117675 w = 0.9275642096687852 b = 46.001474667684406\n",
            "iteration 499 loss = 33512.94795928146 w = 0.9277415275355679 b = 46.00147669583093\n",
            "iteration 500 loss = 33456.58958732183 w = 0.9279185983760192 b = 46.001478721053836\n",
            "iteration 501 loss = 33400.43050552618 w = 0.9280954225342797 b = 46.0014807433572\n",
            "iteration 502 loss = 33344.47009997835 w = 0.9282720003540106 b = 46.001482762745084\n",
            "iteration 503 loss = 33288.70775855395 w = 0.9284483321783945 b = 46.001484779221556\n",
            "iteration 504 loss = 33233.14287091496 w = 0.9286244183501358 b = 46.001486792790665\n",
            "iteration 505 loss = 33177.774828504706 w = 0.9288002592114615 b = 46.00148880345647\n",
            "iteration 506 loss = 33122.60302454293 w = 0.9289758551041217 b = 46.00149081122301\n",
            "iteration 507 loss = 33067.62685402065 w = 0.9291512063693906 b = 46.00149281609432\n",
            "iteration 508 loss = 33012.84571369495 w = 0.9293263133480668 b = 46.00149481807444\n",
            "iteration 509 loss = 32958.25900208417 w = 0.9295011763804742 b = 46.0014968171674\n",
            "iteration 510 loss = 32903.8661194628 w = 0.9296757958064626 b = 46.00149881337722\n",
            "iteration 511 loss = 32849.66646785645 w = 0.9298501719654083 b = 46.00150080670791\n",
            "iteration 512 loss = 32795.65945103692 w = 0.9300243051962148 b = 46.00150279716348\n",
            "iteration 513 loss = 32741.844474517256 w = 0.9301981958373134 b = 46.00150478474795\n",
            "iteration 514 loss = 32688.220945546633 w = 0.9303718442266641 b = 46.00150676946531\n",
            "iteration 515 loss = 32634.788273105514 w = 0.9305452507017559 b = 46.00150875131955\n",
            "iteration 516 loss = 32581.54586790082 w = 0.9307184155996077 b = 46.00151073031466\n",
            "iteration 517 loss = 32528.493142360698 w = 0.9308913392567689 b = 46.00151270645464\n",
            "iteration 518 loss = 32475.629510630053 w = 0.9310640220093201 b = 46.00151467974345\n",
            "iteration 519 loss = 32422.954388565202 w = 0.9312364641928736 b = 46.00151665018507\n",
            "iteration 520 loss = 32370.4671937295 w = 0.9314086661425741 b = 46.001518617783454\n",
            "iteration 521 loss = 32318.16734538783 w = 0.9315806281930995 b = 46.001520582542575\n",
            "iteration 522 loss = 32266.054264502352 w = 0.9317523506786615 b = 46.00152254446639\n",
            "iteration 523 loss = 32214.127373727402 w = 0.931923833933006 b = 46.00152450355884\n",
            "iteration 524 loss = 32162.386097404633 w = 0.9320950782894142 b = 46.00152645982388\n",
            "iteration 525 loss = 32110.82986155832 w = 0.9322660840807028 b = 46.00152841326544\n",
            "iteration 526 loss = 32059.458093890455 w = 0.9324368516392249 b = 46.001530363887454\n",
            "iteration 527 loss = 32008.27022377609 w = 0.9326073812968705 b = 46.00153231169386\n",
            "iteration 528 loss = 31957.26568225846 w = 0.9327776733850675 b = 46.001534256688565\n",
            "iteration 529 loss = 31906.443902044273 w = 0.9329477282347817 b = 46.0015361988755\n",
            "iteration 530 loss = 31855.804317498976 w = 0.9331175461765182 b = 46.00153813825857\n",
            "iteration 531 loss = 31805.346364642 w = 0.9332871275403214 b = 46.00154007484168\n",
            "iteration 532 loss = 31755.069481141996 w = 0.933456472655776 b = 46.00154200862873\n",
            "iteration 533 loss = 31704.973106312336 w = 0.9336255818520076 b = 46.00154393962362\n",
            "iteration 534 loss = 31655.05668110611 w = 0.9337944554576831 b = 46.001545867830245\n",
            "iteration 535 loss = 31605.319648111796 w = 0.9339630938010117 b = 46.00154779325248\n",
            "iteration 536 loss = 31555.761451548286 w = 0.9341314972097453 b = 46.0015497158942\n",
            "iteration 537 loss = 31506.381537260444 w = 0.9342996660111792 b = 46.00155163575929\n",
            "iteration 538 loss = 31457.179352714356 w = 0.9344676005321527 b = 46.00155355285161\n",
            "iteration 539 loss = 31408.154346992756 w = 0.9346353010990499 b = 46.00155546717503\n",
            "iteration 540 loss = 31359.305970790487 w = 0.9348027680378 b = 46.0015573787334\n",
            "iteration 541 loss = 31310.633676409747 w = 0.9349700016738783 b = 46.001559287530576\n",
            "iteration 542 loss = 31262.13691775562 w = 0.9351370023323066 b = 46.0015611935704\n",
            "iteration 543 loss = 31213.81515033141 w = 0.9353037703376538 b = 46.00156309685673\n",
            "iteration 544 loss = 31165.667831234205 w = 0.9354703060140368 b = 46.00156499739338\n",
            "iteration 545 loss = 31117.694419150306 w = 0.935636609685121 b = 46.00156689518419\n",
            "iteration 546 loss = 31069.89437435051 w = 0.9358026816741207 b = 46.00156879023299\n",
            "iteration 547 loss = 31022.267158685838 w = 0.9359685223038001 b = 46.001570682543594\n",
            "iteration 548 loss = 30974.812235582944 w = 0.9361341318964737 b = 46.00157257211982\n",
            "iteration 549 loss = 30927.52907003957 w = 0.9362995107740067 b = 46.00157445896547\n",
            "iteration 550 loss = 30880.41712862013 w = 0.9364646592578164 b = 46.00157634308436\n",
            "iteration 551 loss = 30833.475879451154 w = 0.9366295776688719 b = 46.001578224480276\n",
            "iteration 552 loss = 30786.704792216813 w = 0.9367942663276954 b = 46.001580103157025\n",
            "iteration 553 loss = 30740.10333815478 w = 0.9369587255543623 b = 46.001581979118384\n",
            "iteration 554 loss = 30693.670990051374 w = 0.9371229556685025 b = 46.00158385236814\n",
            "iteration 555 loss = 30647.407222237314 w = 0.9372869569893002 b = 46.00158572291007\n",
            "iteration 556 loss = 30601.311510583473 w = 0.9374507298354952 b = 46.00158759074795\n",
            "iteration 557 loss = 30555.38333249625 w = 0.9376142745253832 b = 46.001589455885544\n",
            "iteration 558 loss = 30509.622166913276 w = 0.9377775913768163 b = 46.00159131832661\n",
            "iteration 559 loss = 30464.027494299033 w = 0.937940680707204 b = 46.00159317807491\n",
            "iteration 560 loss = 30418.59879664061 w = 0.9381035428335136 b = 46.0015950351342\n",
            "iteration 561 loss = 30373.335557443108 w = 0.9382661780722706 b = 46.001596889508214\n",
            "iteration 562 loss = 30328.237261725597 w = 0.9384285867395599 b = 46.0015987412007\n",
            "iteration 563 loss = 30283.30339601662 w = 0.9385907691510256 b = 46.001600590215396\n",
            "iteration 564 loss = 30238.53344834982 w = 0.9387527256218725 b = 46.00160243655603\n",
            "iteration 565 loss = 30193.92690825989 w = 0.938914456466866 b = 46.00160428022632\n",
            "iteration 566 loss = 30149.483266778047 w = 0.939075962000333 b = 46.00160612122999\n",
            "iteration 567 loss = 30105.202016427866 w = 0.9392372425361628 b = 46.00160795957076\n",
            "iteration 568 loss = 30061.082651221077 w = 0.9393982983878071 b = 46.00160979525234\n",
            "iteration 569 loss = 30017.12466665313 w = 0.9395591298682809 b = 46.00161162827843\n",
            "iteration 570 loss = 29973.32755969924 w = 0.9397197372901633 b = 46.001613458652734\n",
            "iteration 571 loss = 29929.690828809886 w = 0.9398801209655979 b = 46.00161528637894\n",
            "iteration 572 loss = 29886.21397390675 w = 0.9400402812062933 b = 46.001617111460746\n",
            "iteration 573 loss = 29842.89649637845 w = 0.9402002183235242 b = 46.00161893390183\n",
            "iteration 574 loss = 29799.737899076412 w = 0.9403599326281312 b = 46.00162075370587\n",
            "iteration 575 loss = 29756.737686310717 w = 0.9405194244305222 b = 46.00162257087655\n",
            "iteration 576 loss = 29713.89536384577 w = 0.9406786940406724 b = 46.00162438541752\n",
            "iteration 577 loss = 29671.210438896418 w = 0.9408377417681253 b = 46.00162619733246\n",
            "iteration 578 loss = 29628.682420123452 w = 0.9409965679219933 b = 46.001628006625026\n",
            "iteration 579 loss = 29586.310817629856 w = 0.9411551728109578 b = 46.00162981329886\n",
            "iteration 580 loss = 29544.095142956452 w = 0.9413135567432707 b = 46.001631617357624\n",
            "iteration 581 loss = 29502.034909077724 w = 0.9414717200267539 b = 46.00163341880495\n",
            "iteration 582 loss = 29460.129630398085 w = 0.9416296629688009 b = 46.001635217644484\n",
            "iteration 583 loss = 29418.378822747392 w = 0.9417873858763767 b = 46.00163701387985\n",
            "iteration 584 loss = 29376.782003377106 w = 0.9419448890560186 b = 46.00163880751469\n",
            "iteration 585 loss = 29335.33869095611 w = 0.9421021728138371 b = 46.00164059855261\n",
            "iteration 586 loss = 29294.0484055668 w = 0.9422592374555162 b = 46.00164238699724\n",
            "iteration 587 loss = 29252.91066870091 w = 0.9424160832863139 b = 46.00164417285219\n",
            "iteration 588 loss = 29211.92500325557 w = 0.942572710611063 b = 46.00164595612107\n",
            "iteration 589 loss = 29171.090933529154 w = 0.9427291197341716 b = 46.00164773680748\n",
            "iteration 590 loss = 29130.407985217575 w = 0.9428853109596239 b = 46.001649514915016\n",
            "iteration 591 loss = 29089.875685409963 w = 0.9430412845909804 b = 46.00165129044727\n",
            "iteration 592 loss = 29049.49356258488 w = 0.9431970409313787 b = 46.001653063407836\n",
            "iteration 593 loss = 29009.26114660626 w = 0.9433525802835342 b = 46.001654833800295\n",
            "iteration 594 loss = 28969.177968719483 w = 0.9435079029497405 b = 46.00165660162822\n",
            "iteration 595 loss = 28929.243561547468 w = 0.9436630092318702 b = 46.001658366895185\n",
            "iteration 596 loss = 28889.457459086712 w = 0.9438178994313751 b = 46.00166012960476\n",
            "iteration 597 loss = 28849.81919670327 w = 0.9439725738492872 b = 46.001661889760506\n",
            "iteration 598 loss = 28810.328311128967 w = 0.9441270327862192 b = 46.00166364736598\n",
            "iteration 599 loss = 28770.984340457446 w = 0.9442812765423649 b = 46.00166540242474\n",
            "iteration 600 loss = 28731.7868241403 w = 0.9444353054174999 b = 46.00166715494033\n",
            "iteration 601 loss = 28692.735302983147 w = 0.944589119710982 b = 46.00166890491629\n",
            "iteration 602 loss = 28653.82931914181 w = 0.9447427197217524 b = 46.00167065235617\n",
            "iteration 603 loss = 28615.06841611838 w = 0.9448961057483354 b = 46.001672397263484\n",
            "iteration 604 loss = 28576.452138757508 w = 0.9450492780888395 b = 46.00167413964178\n",
            "iteration 605 loss = 28537.98003324234 w = 0.9452022370409582 b = 46.00167587949456\n",
            "iteration 606 loss = 28499.651647090985 w = 0.9453549829019698 b = 46.00167761682537\n",
            "iteration 607 loss = 28461.46652915244 w = 0.9455075159687389 b = 46.0016793516377\n",
            "iteration 608 loss = 28423.424229602886 w = 0.9456598365377163 b = 46.00168108393506\n",
            "iteration 609 loss = 28385.524299942004 w = 0.9458119449049398 b = 46.00168281372097\n",
            "iteration 610 loss = 28347.76629298894 w = 0.9459638413660347 b = 46.001684540998916\n",
            "iteration 611 loss = 28310.149762878755 w = 0.9461155262162148 b = 46.00168626577239\n",
            "iteration 612 loss = 28272.674265058577 w = 0.9462669997502822 b = 46.00168798804489\n",
            "iteration 613 loss = 28235.33935628393 w = 0.9464182622626285 b = 46.0016897078199\n",
            "iteration 614 loss = 28198.14459461473 w = 0.9465693140472352 b = 46.001691425100894\n",
            "iteration 615 loss = 28161.089539411947 w = 0.9467201553976743 b = 46.001693139891344\n",
            "iteration 616 loss = 28124.173751333485 w = 0.9468707866071087 b = 46.001694852194724\n",
            "iteration 617 loss = 28087.396792330863 w = 0.9470212079682928 b = 46.0016965620145\n",
            "iteration 618 loss = 28050.758225645197 w = 0.9471714197735734 b = 46.00169826935413\n",
            "iteration 619 loss = 28014.257615803654 w = 0.9473214223148898 b = 46.00169997421707\n",
            "iteration 620 loss = 27977.894528615812 w = 0.9474712158837747 b = 46.00170167660677\n",
            "iteration 621 loss = 27941.668531169846 w = 0.9476208007713547 b = 46.00170337652668\n",
            "iteration 622 loss = 27905.579191829027 w = 0.9477701772683507 b = 46.00170507398023\n",
            "iteration 623 loss = 27869.626080228063 w = 0.9479193456650785 b = 46.00170676897087\n",
            "iteration 624 loss = 27833.808767269216 w = 0.9480683062514499 b = 46.00170846150202\n",
            "iteration 625 loss = 27798.126825119103 w = 0.9482170593169722 b = 46.00171015157711\n",
            "iteration 626 loss = 27762.57982720467 w = 0.9483656051507499 b = 46.00171183919956\n",
            "iteration 627 loss = 27727.167348209812 w = 0.9485139440414844 b = 46.00171352437278\n",
            "iteration 628 loss = 27691.888964071823 w = 0.948662076277475 b = 46.0017152071002\n",
            "iteration 629 loss = 27656.744251977452 w = 0.9488100021466196 b = 46.001716887385214\n",
            "iteration 630 loss = 27621.73279035993 w = 0.9489577219364146 b = 46.00171856523123\n",
            "iteration 631 loss = 27586.854158894756 w = 0.9491052359339561 b = 46.001720240641646\n",
            "iteration 632 loss = 27552.107938496676 w = 0.9492525444259403 b = 46.00172191361985\n",
            "iteration 633 loss = 27517.49371131574 w = 0.9493996476986639 b = 46.00172358416924\n",
            "iteration 634 loss = 27483.01106073402 w = 0.9495465460380247 b = 46.00172525229319\n",
            "iteration 635 loss = 27448.65957136202 w = 0.9496932397295224 b = 46.00172691799508\n",
            "iteration 636 loss = 27414.438829035065 w = 0.9498397290582586 b = 46.001728581278286\n",
            "iteration 637 loss = 27380.3484208099 w = 0.9499860143089383 b = 46.00173024214618\n",
            "iteration 638 loss = 27346.387934961236 w = 0.9501320957658691 b = 46.001731900602124\n",
            "iteration 639 loss = 27312.55696097808 w = 0.9502779737129632 b = 46.00173355664948\n",
            "iteration 640 loss = 27278.855089560475 w = 0.950423648433737 b = 46.0017352102916\n",
            "iteration 641 loss = 27245.28191261577 w = 0.9505691202113118 b = 46.00173686153184\n",
            "iteration 642 loss = 27211.837023255513 w = 0.9507143893284147 b = 46.00173851037354\n",
            "iteration 643 loss = 27178.520015791702 w = 0.9508594560673788 b = 46.00174015682005\n",
            "iteration 644 loss = 27145.330485733502 w = 0.951004320710144 b = 46.0017418008747\n",
            "iteration 645 loss = 27112.268029783747 w = 0.9511489835382572 b = 46.00174344254082\n",
            "iteration 646 loss = 27079.332245835565 w = 0.9512934448328731 b = 46.001745081821745\n",
            "iteration 647 loss = 27046.52273296902 w = 0.9514377048747551 b = 46.001746718720796\n",
            "iteration 648 loss = 27013.83909144747 w = 0.951581763944275 b = 46.00174835324129\n",
            "iteration 649 loss = 26981.280922714577 w = 0.9517256223214141 b = 46.00174998538654\n",
            "iteration 650 loss = 26948.84782939059 w = 0.9518692802857638 b = 46.001751615159854\n",
            "iteration 651 loss = 26916.539415269126 w = 0.9520127381165259 b = 46.00175324256454\n",
            "iteration 652 loss = 26884.355285313744 w = 0.9521559960925132 b = 46.00175486760389\n",
            "iteration 653 loss = 26852.295045654664 w = 0.9522990544921501 b = 46.00175649028121\n",
            "iteration 654 loss = 26820.35830358537 w = 0.9524419135934733 b = 46.00175811059979\n",
            "iteration 655 loss = 26788.544667559316 w = 0.9525845736741319 b = 46.0017597285629\n",
            "iteration 656 loss = 26756.853747186564 w = 0.9527270350113881 b = 46.00176134417384\n",
            "iteration 657 loss = 26725.285153230565 w = 0.9528692978821183 b = 46.001762957435886\n",
            "iteration 658 loss = 26693.838497604673 w = 0.9530113625628127 b = 46.0017645683523\n",
            "iteration 659 loss = 26662.513393369038 w = 0.9531532293295767 b = 46.00176617692635\n",
            "iteration 660 loss = 26631.309454727212 w = 0.9532948984581308 b = 46.001767783161306\n",
            "iteration 661 loss = 26600.2262970229 w = 0.9534363702238113 b = 46.00176938706042\n",
            "iteration 662 loss = 26569.26353673676 w = 0.9535776449015714 b = 46.00177098862695\n",
            "iteration 663 loss = 26538.420791483033 w = 0.9537187227659806 b = 46.00177258786415\n",
            "iteration 664 loss = 26507.697680006313 w = 0.9538596040912263 b = 46.001774184775265\n",
            "iteration 665 loss = 26477.093822178427 w = 0.9540002891511139 b = 46.00177577936353\n",
            "iteration 666 loss = 26446.608838995064 w = 0.9541407782190672 b = 46.00177737163217\n",
            "iteration 667 loss = 26416.242352572626 w = 0.9542810715681292 b = 46.00177896158444\n",
            "iteration 668 loss = 26385.993986144997 w = 0.9544211694709626 b = 46.001780549223554\n",
            "iteration 669 loss = 26355.86336406034 w = 0.9545610721998499 b = 46.001782134552734\n",
            "iteration 670 loss = 26325.85011177795 w = 0.9547007800266946 b = 46.0017837175752\n",
            "iteration 671 loss = 26295.953855864922 w = 0.9548402932230212 b = 46.001785298294166\n",
            "iteration 672 loss = 26266.17422399323 w = 0.9549796120599763 b = 46.00178687671284\n",
            "iteration 673 loss = 26236.510844936285 w = 0.9551187368083283 b = 46.00178845283443\n",
            "iteration 674 loss = 26206.963348566012 w = 0.9552576677384685 b = 46.001790026662135\n",
            "iteration 675 loss = 26177.531365849467 w = 0.9553964051204118 b = 46.00179159819915\n",
            "iteration 676 loss = 26148.214528845947 w = 0.9555349492237964 b = 46.00179316744866\n",
            "iteration 677 loss = 26119.01247070366 w = 0.9556733003178854 b = 46.00179473441386\n",
            "iteration 678 loss = 26089.92482565667 w = 0.9558114586715666 b = 46.00179629909794\n",
            "iteration 679 loss = 26060.95122902185 w = 0.955949424553353 b = 46.00179786150406\n",
            "iteration 680 loss = 26032.091317195773 w = 0.9560871982313837 b = 46.001799421635404\n",
            "iteration 681 loss = 26003.344727651413 w = 0.9562247799734243 b = 46.00180097949514\n",
            "iteration 682 loss = 25974.71109893527 w = 0.9563621700468672 b = 46.001802535086426\n",
            "iteration 683 loss = 25946.190070664274 w = 0.9564993687187325 b = 46.00180408841243\n",
            "iteration 684 loss = 25917.781283522647 w = 0.9566363762556681 b = 46.0018056394763\n",
            "iteration 685 loss = 25889.48437925885 w = 0.9567731929239506 b = 46.0018071882812\n",
            "iteration 686 loss = 25861.299000682593 w = 0.9569098189894855 b = 46.00180873483026\n",
            "iteration 687 loss = 25833.224791661705 w = 0.9570462547178078 b = 46.001810279126644\n",
            "iteration 688 loss = 25805.26139711911 w = 0.9571825003740828 b = 46.001811821173476\n",
            "iteration 689 loss = 25777.408463029948 w = 0.9573185562231062 b = 46.00181336097389\n",
            "iteration 690 loss = 25749.66563641836 w = 0.957454422529305 b = 46.00181489853102\n",
            "iteration 691 loss = 25722.032565354562 w = 0.9575900995567376 b = 46.00181643384799\n",
            "iteration 692 loss = 25694.50889895185 w = 0.9577255875690945 b = 46.00181796692792\n",
            "iteration 693 loss = 25667.09428736359 w = 0.9578608868296993 b = 46.00181949777393\n",
            "iteration 694 loss = 25639.78838178028 w = 0.9579959976015081 b = 46.00182102638912\n",
            "iteration 695 loss = 25612.590834426464 w = 0.9581309201471112 b = 46.001822552776616\n",
            "iteration 696 loss = 25585.501298558014 w = 0.9582656547287327 b = 46.001824076939506\n",
            "iteration 697 loss = 25558.51942845876 w = 0.9584002016082317 b = 46.0018255988809\n",
            "iteration 698 loss = 25531.644879437936 w = 0.9585345610471022 b = 46.001827118603885\n",
            "iteration 699 loss = 25504.877307826995 w = 0.9586687333064742 b = 46.001828636111554\n",
            "iteration 700 loss = 25478.216370976897 w = 0.9588027186471136 b = 46.00183015140699\n",
            "iteration 701 loss = 25451.661727254836 w = 0.9589365173294234 b = 46.00183166449329\n",
            "iteration 702 loss = 25425.21303604169 w = 0.9590701296134435 b = 46.00183317537351\n",
            "iteration 703 loss = 25398.869957728897 w = 0.9592035557588516 b = 46.00183468405074\n",
            "iteration 704 loss = 25372.63215371568 w = 0.9593367960249638 b = 46.001836190528046\n",
            "iteration 705 loss = 25346.49928640601 w = 0.9594698506707346 b = 46.001837694808486\n",
            "iteration 706 loss = 25320.471019205794 w = 0.9596027199547582 b = 46.001839196895126\n",
            "iteration 707 loss = 25294.54701652007 w = 0.959735404135268 b = 46.00184069679102\n",
            "iteration 708 loss = 25268.726943750073 w = 0.9598679034701382 b = 46.00184219449922\n",
            "iteration 709 loss = 25243.010467290253 w = 0.9600002182168833 b = 46.001843690022774\n",
            "iteration 710 loss = 25217.397254525673 w = 0.9601323486326594 b = 46.00184518336472\n",
            "iteration 711 loss = 25191.886973829016 w = 0.960264294974264 b = 46.00184667452811\n",
            "iteration 712 loss = 25166.47929455773 w = 0.9603960574981372 b = 46.00184816351597\n",
            "iteration 713 loss = 25141.173887051275 w = 0.9605276364603615 b = 46.00184965033134\n",
            "iteration 714 loss = 25115.970422628256 w = 0.960659032116663 b = 46.00185113497723\n",
            "iteration 715 loss = 25090.868573583666 w = 0.9607902447224113 b = 46.00185261745667\n",
            "iteration 716 loss = 25065.868013185962 w = 0.9609212745326202 b = 46.001854097772686\n",
            "iteration 717 loss = 25040.968415674415 w = 0.9610521218019484 b = 46.00185557592828\n",
            "iteration 718 loss = 25016.1694562562 w = 0.9611827867846998 b = 46.00185705192647\n",
            "iteration 719 loss = 24991.470811103758 w = 0.9613132697348239 b = 46.00185852577026\n",
            "iteration 720 loss = 24966.872157351852 w = 0.9614435709059165 b = 46.00185999746265\n",
            "iteration 721 loss = 24942.373173094926 w = 0.96157369055122 b = 46.00186146700664\n",
            "iteration 722 loss = 24917.973537384343 w = 0.9617036289236242 b = 46.00186293440522\n",
            "iteration 723 loss = 24893.672930225544 w = 0.9618333862756664 b = 46.00186439966138\n",
            "iteration 724 loss = 24869.471032575337 w = 0.9619629628595323 b = 46.001865862778104\n",
            "iteration 725 loss = 24845.367526339367 w = 0.962092358927056 b = 46.00186732375837\n",
            "iteration 726 loss = 24821.362094368964 w = 0.962221574729721 b = 46.00186878260516\n",
            "iteration 727 loss = 24797.45442045895 w = 0.9623506105186603 b = 46.00187023932144\n",
            "iteration 728 loss = 24773.644189344424 w = 0.9624794665446571 b = 46.00187169391018\n",
            "iteration 729 loss = 24749.931086698474 w = 0.9626081430581451 b = 46.00187314637435\n",
            "iteration 730 loss = 24726.314799129184 w = 0.9627366403092094 b = 46.0018745967169\n",
            "iteration 731 loss = 24702.795014177053 w = 0.9628649585475864 b = 46.0018760449408\n",
            "iteration 732 loss = 24679.371420312433 w = 0.9629930980226648 b = 46.00187749104899\n",
            "iteration 733 loss = 24656.043706932684 w = 0.9631210589834857 b = 46.00187893504442\n",
            "iteration 734 loss = 24632.81156435957 w = 0.9632488416787433 b = 46.00188037693003\n",
            "iteration 735 loss = 24609.674683836696 w = 0.9633764463567853 b = 46.00188181670877\n",
            "iteration 736 loss = 24586.632757526604 w = 0.9635038732656137 b = 46.00188325438356\n",
            "iteration 737 loss = 24563.68547850847 w = 0.9636311226528846 b = 46.001884689957336\n",
            "iteration 738 loss = 24540.83254077518 w = 0.9637581947659094 b = 46.00188612343303\n",
            "iteration 739 loss = 24518.073639230883 w = 0.9638850898516547 b = 46.00188755481357\n",
            "iteration 740 loss = 24495.408469688307 w = 0.9640118081567431 b = 46.00188898410187\n",
            "iteration 741 loss = 24472.836728866114 w = 0.964138349927454 b = 46.001890411300835\n",
            "iteration 742 loss = 24450.35811438633 w = 0.964264715409723 b = 46.001891836413385\n",
            "iteration 743 loss = 24427.97232477178 w = 0.9643909048491438 b = 46.001893259442426\n",
            "iteration 744 loss = 24405.679059443548 w = 0.9645169184909674 b = 46.001894680390855\n",
            "iteration 745 loss = 24383.4780187181 w = 0.9646427565801035 b = 46.00189609926158\n",
            "iteration 746 loss = 24361.3689038051 w = 0.9647684193611203 b = 46.00189751605749\n",
            "iteration 747 loss = 24339.351416804595 w = 0.9648939070782456 b = 46.00189893078147\n",
            "iteration 748 loss = 24317.425260704553 w = 0.9650192199753668 b = 46.00190034343642\n",
            "iteration 749 loss = 24295.590139378248 w = 0.9651443582960314 b = 46.001901754025205\n",
            "iteration 750 loss = 24273.845757581734 w = 0.9652693222834479 b = 46.00190316255072\n",
            "iteration 751 loss = 24252.19182095139 w = 0.9653941121804857 b = 46.00190456901583\n",
            "iteration 752 loss = 24230.62803600122 w = 0.9655187282296759 b = 46.001905973423405\n",
            "iteration 753 loss = 24209.154110120515 w = 0.9656431706732119 b = 46.00190737577632\n",
            "iteration 754 loss = 24187.769751571133 w = 0.9657674397529497 b = 46.00190877607743\n",
            "iteration 755 loss = 24166.474669485175 w = 0.9658915357104081 b = 46.001910174329595\n",
            "iteration 756 loss = 24145.268573862384 w = 0.9660154587867696 b = 46.00191157053567\n",
            "iteration 757 loss = 24124.151175567742 w = 0.9661392092228808 b = 46.001912964698505\n",
            "iteration 758 loss = 24103.122186328772 w = 0.9662627872592524 b = 46.00191435682095\n",
            "iteration 759 loss = 24082.181318733277 w = 0.9663861931360604 b = 46.00191574690584\n",
            "iteration 760 loss = 24061.328286226777 w = 0.9665094270931462 b = 46.00191713495602\n",
            "iteration 761 loss = 24040.56280311001 w = 0.9666324893700169 b = 46.00191852097432\n",
            "iteration 762 loss = 24019.88458453656 w = 0.9667553802058458 b = 46.00191990496358\n",
            "iteration 763 loss = 23999.293346510294 w = 0.9668780998394735 b = 46.00192128692662\n",
            "iteration 764 loss = 23978.788805882934 w = 0.9670006485094074 b = 46.00192266686626\n",
            "iteration 765 loss = 23958.37068035181 w = 0.9671230264538226 b = 46.00192404478532\n",
            "iteration 766 loss = 23938.03868845704 w = 0.9672452339105629 b = 46.00192542068662\n",
            "iteration 767 loss = 23917.792549579535 w = 0.9673672711171403 b = 46.00192679457297\n",
            "iteration 768 loss = 23897.63198393827 w = 0.9674891383107359 b = 46.001928166447165\n",
            "iteration 769 loss = 23877.556712587997 w = 0.9676108357282007 b = 46.00192953631202\n",
            "iteration 770 loss = 23857.566457416713 w = 0.9677323636060555 b = 46.00193090417034\n",
            "iteration 771 loss = 23837.660941143615 w = 0.9678537221804916 b = 46.0019322700249\n",
            "iteration 772 loss = 23817.839887316248 w = 0.9679749116873713 b = 46.001933633878515\n",
            "iteration 773 loss = 23798.103020308343 w = 0.9680959323622282 b = 46.00193499573396\n",
            "iteration 774 loss = 23778.45006531759 w = 0.9682167844402682 b = 46.00193635559402\n",
            "iteration 775 loss = 23758.88074836286 w = 0.9683374681563688 b = 46.00193771346147\n",
            "iteration 776 loss = 23739.39479628224 w = 0.968457983745081 b = 46.001939069339095\n",
            "iteration 777 loss = 23719.991936730443 w = 0.9685783314406285 b = 46.00194042322966\n",
            "iteration 778 loss = 23700.671898176653 w = 0.9686985114769091 b = 46.00194177513594\n",
            "iteration 779 loss = 23681.43440990182 w = 0.9688185240874945 b = 46.0019431250607\n",
            "iteration 780 loss = 23662.279201996793 w = 0.9689383695056311 b = 46.00194447300669\n",
            "iteration 781 loss = 23643.206005359618 w = 0.9690580479642402 b = 46.00194581897667\n",
            "iteration 782 loss = 23624.214551693338 w = 0.969177559695919 b = 46.0019471629734\n",
            "iteration 783 loss = 23605.304573503712 w = 0.9692969049329403 b = 46.001948504999625\n",
            "iteration 784 loss = 23586.47580409692 w = 0.9694160839072533 b = 46.00194984505809\n",
            "iteration 785 loss = 23567.72797757711 w = 0.9695350968504842 b = 46.00195118315154\n",
            "iteration 786 loss = 23549.060828844224 w = 0.9696539439939367 b = 46.00195251928271\n",
            "iteration 787 loss = 23530.474093591776 w = 0.9697726255685918 b = 46.00195385345433\n",
            "iteration 788 loss = 23511.967508304297 w = 0.969891141805109 b = 46.00195518566913\n",
            "iteration 789 loss = 23493.540810255367 w = 0.9700094929338267 b = 46.00195651592984\n",
            "iteration 790 loss = 23475.193737505044 w = 0.9701276791847618 b = 46.001957844239186\n",
            "iteration 791 loss = 23456.926028898 w = 0.9702457007876113 b = 46.001959170599875\n",
            "iteration 792 loss = 23438.737424060764 w = 0.9703635579717519 b = 46.00196049501463\n",
            "iteration 793 loss = 23420.6276633998 w = 0.9704812509662408 b = 46.00196181748616\n",
            "iteration 794 loss = 23402.596488099236 w = 0.9705987799998163 b = 46.001963138017175\n",
            "iteration 795 loss = 23384.64364011849 w = 0.9707161453008977 b = 46.001964456610374\n",
            "iteration 796 loss = 23366.768862190096 w = 0.9708333470975863 b = 46.00196577326846\n",
            "iteration 797 loss = 23348.971897817613 w = 0.9709503856176657 b = 46.001967087994124\n",
            "iteration 798 loss = 23331.252491273077 w = 0.9710672610886019 b = 46.00196840079006\n",
            "iteration 799 loss = 23313.610387595156 w = 0.9711839737375442 b = 46.001969711658965\n",
            "iteration 800 loss = 23296.045332586666 w = 0.9713005237913254 b = 46.00197102060351\n",
            "iteration 801 loss = 23278.55707281252 w = 0.9714169114764623 b = 46.00197232762639\n",
            "iteration 802 loss = 23261.1453555974 w = 0.9715331370191561 b = 46.00197363273027\n",
            "iteration 803 loss = 23243.80992902374 w = 0.971649200645293 b = 46.00197493591783\n",
            "iteration 804 loss = 23226.550541929486 w = 0.9717651025804444 b = 46.001976237191734\n",
            "iteration 805 loss = 23209.36694390567 w = 0.9718808430498672 b = 46.001977536554655\n",
            "iteration 806 loss = 23192.2588852947 w = 0.971996422278505 b = 46.00197883400925\n",
            "iteration 807 loss = 23175.226117187703 w = 0.9721118404909878 b = 46.00198012955818\n",
            "iteration 808 loss = 23158.2683914228 w = 0.9722270979116324 b = 46.00198142320409\n",
            "iteration 809 loss = 23141.38546058259 w = 0.9723421947644436 b = 46.00198271494965\n",
            "iteration 810 loss = 23124.577077992286 w = 0.9724571312731137 b = 46.00198400479749\n",
            "iteration 811 loss = 23107.842997717336 w = 0.9725719076610235 b = 46.00198529275027\n",
            "iteration 812 loss = 23091.182974561572 w = 0.9726865241512427 b = 46.001986578810616\n",
            "iteration 813 loss = 23074.596764064692 w = 0.9728009809665301 b = 46.001987862981174\n",
            "iteration 814 loss = 23058.084122500495 w = 0.9729152783293343 b = 46.001989145264574\n",
            "iteration 815 loss = 23041.644806874636 w = 0.9730294164617939 b = 46.00199042566344\n",
            "iteration 816 loss = 23025.2785749224 w = 0.9731433955857379 b = 46.001991704180405\n",
            "iteration 817 loss = 23008.98518510679 w = 0.9732572159226867 b = 46.00199298081809\n",
            "iteration 818 loss = 22992.764396616265 w = 0.9733708776938516 b = 46.001994255579106\n",
            "iteration 819 loss = 22976.61596936277 w = 0.9734843811201359 b = 46.00199552846607\n",
            "iteration 820 loss = 22960.539663979525 w = 0.9735977264221354 b = 46.001996799481596\n",
            "iteration 821 loss = 22944.535241819103 w = 0.9737109138201382 b = 46.00199806862829\n",
            "iteration 822 loss = 22928.602464951146 w = 0.9738239435341258 b = 46.00199933590876\n",
            "iteration 823 loss = 22912.741096160524 w = 0.973936815783773 b = 46.00200060132559\n",
            "iteration 824 loss = 22896.950898945077 w = 0.9740495307884488 b = 46.00200186488139\n",
            "iteration 825 loss = 22881.231637513705 w = 0.9741620887672163 b = 46.00200312657876\n",
            "iteration 826 loss = 22865.583076784184 w = 0.9742744899388337 b = 46.00200438642027\n",
            "iteration 827 loss = 22850.00498238113 w = 0.9743867345217543 b = 46.00200564440851\n",
            "iteration 828 loss = 22834.497120634187 w = 0.974498822734127 b = 46.00200690054608\n",
            "iteration 829 loss = 22819.05925857565 w = 0.9746107547937971 b = 46.002008154835536\n",
            "iteration 830 loss = 22803.691163938667 w = 0.974722530918306 b = 46.00200940727946\n",
            "iteration 831 loss = 22788.39260515511 w = 0.9748341513248923 b = 46.002010657880426\n",
            "iteration 832 loss = 22773.163351353654 w = 0.9749456162304918 b = 46.002011906641\n",
            "iteration 833 loss = 22758.003172357683 w = 0.9750569258517382 b = 46.002013153563745\n",
            "iteration 834 loss = 22742.91183868329 w = 0.9751680804049634 b = 46.00201439865122\n",
            "iteration 835 loss = 22727.889121537235 w = 0.9752790801061979 b = 46.002015641905984\n",
            "iteration 836 loss = 22712.93479281509 w = 0.9753899251711712 b = 46.00201688333059\n",
            "iteration 837 loss = 22698.04862509918 w = 0.9755006158153122 b = 46.00201812292759\n",
            "iteration 838 loss = 22683.230391656485 w = 0.97561115225375 b = 46.00201936069953\n",
            "iteration 839 loss = 22668.47986643686 w = 0.9757215347013134 b = 46.00202059664895\n",
            "iteration 840 loss = 22653.796824070916 w = 0.9758317633725325 b = 46.00202183077838\n",
            "iteration 841 loss = 22639.181039868057 w = 0.9759418384816384 b = 46.00202306309038\n",
            "iteration 842 loss = 22624.632289814643 w = 0.9760517602425635 b = 46.00202429358745\n",
            "iteration 843 loss = 22610.15035057193 w = 0.9761615288689423 b = 46.00202552227215\n",
            "iteration 844 loss = 22595.73499947412 w = 0.9762711445741117 b = 46.002026749146985\n",
            "iteration 845 loss = 22581.38601452645 w = 0.9763806075711114 b = 46.00202797421448\n",
            "iteration 846 loss = 22567.10317440322 w = 0.9764899180726845 b = 46.00202919747715\n",
            "iteration 847 loss = 22552.886258445917 w = 0.9765990762912772 b = 46.002030418937515\n",
            "iteration 848 loss = 22538.735046661204 w = 0.9767080824390402 b = 46.00203163859809\n",
            "iteration 849 loss = 22524.649319719065 w = 0.9768169367278284 b = 46.002032856461376\n",
            "iteration 850 loss = 22510.628858950873 w = 0.9769256393692018 b = 46.002034072529874\n",
            "iteration 851 loss = 22496.673446347457 w = 0.9770341905744255 b = 46.00203528680609\n",
            "iteration 852 loss = 22482.782864557183 w = 0.9771425905544703 b = 46.00203649929252\n",
            "iteration 853 loss = 22468.95689688414 w = 0.977250839520013 b = 46.00203770999166\n",
            "iteration 854 loss = 22455.195327286136 w = 0.9773589376814371 b = 46.002038918905996\n",
            "iteration 855 loss = 22441.497940372847 w = 0.9774668852488327 b = 46.00204012603801\n",
            "iteration 856 loss = 22427.864521403957 w = 0.9775746824319976 b = 46.002041331390195\n",
            "iteration 857 loss = 22414.294856287248 w = 0.9776823294404372 b = 46.00204253496502\n",
            "iteration 858 loss = 22400.788731576744 w = 0.9777898264833648 b = 46.00204373676497\n",
            "iteration 859 loss = 22387.34593447083 w = 0.9778971737697024 b = 46.00204493679251\n",
            "iteration 860 loss = 22373.966252810384 w = 0.9780043715080811 b = 46.002046135050115\n",
            "iteration 861 loss = 22360.649475076956 w = 0.978111419906841 b = 46.00204733154025\n",
            "iteration 862 loss = 22347.39539039084 w = 0.9782183191740323 b = 46.00204852626537\n",
            "iteration 863 loss = 22334.20378850926 w = 0.9783250695174153 b = 46.00204971922795\n",
            "iteration 864 loss = 22321.074459824657 w = 0.9784316711444605 b = 46.00205091043043\n",
            "iteration 865 loss = 22308.00719536258 w = 0.9785381242623499 b = 46.002052099875264\n",
            "iteration 866 loss = 22295.0017867801 w = 0.9786444290779764 b = 46.00205328756491\n",
            "iteration 867 loss = 22282.05802636384 w = 0.978750585797945 b = 46.00205447350181\n",
            "iteration 868 loss = 22269.17570702826 w = 0.9788565946285728 b = 46.0020556576884\n",
            "iteration 869 loss = 22256.35462231373 w = 0.9789624557758893 b = 46.002056840127125\n",
            "iteration 870 loss = 22243.594566384792 w = 0.979068169445637 b = 46.002058020820414\n",
            "iteration 871 loss = 22230.895334028297 w = 0.9791737358432722 b = 46.0020591997707\n",
            "iteration 872 loss = 22218.256720651712 w = 0.9792791551739644 b = 46.00206037698042\n",
            "iteration 873 loss = 22205.678522281196 w = 0.9793844276425975 b = 46.002061552451984\n",
            "iteration 874 loss = 22193.16053555984 w = 0.9794895534537701 b = 46.00206272618782\n",
            "iteration 875 loss = 22180.702557746008 w = 0.9795945328117955 b = 46.00206389819036\n",
            "iteration 876 loss = 22168.304386711312 w = 0.9796993659207027 b = 46.002065068461995\n",
            "iteration 877 loss = 22155.9658209391 w = 0.9798040529842361 b = 46.00206623700515\n",
            "iteration 878 loss = 22143.686659522496 w = 0.9799085942058565 b = 46.00206740382223\n",
            "iteration 879 loss = 22131.46670216267 w = 0.9800129897887413 b = 46.00206856891564\n",
            "iteration 880 loss = 22119.305749167193 w = 0.9801172399357846 b = 46.00206973228778\n",
            "iteration 881 loss = 22107.20360144814 w = 0.9802213448495979 b = 46.002070893941045\n",
            "iteration 882 loss = 22095.16006052042 w = 0.9803253047325106 b = 46.00207205387784\n",
            "iteration 883 loss = 22083.17492849994 w = 0.9804291197865701 b = 46.00207321210055\n",
            "iteration 884 loss = 22071.24800810203 w = 0.9805327902135424 b = 46.00207436861156\n",
            "iteration 885 loss = 22059.379102639494 w = 0.9806363162149124 b = 46.00207552341326\n",
            "iteration 886 loss = 22047.568016021087 w = 0.9807396979918842 b = 46.002076676508025\n",
            "iteration 887 loss = 22035.81455274964 w = 0.9808429357453816 b = 46.00207782789824\n",
            "iteration 888 loss = 22024.118517920346 w = 0.9809460296760487 b = 46.00207897758628\n",
            "iteration 889 loss = 22012.47971721923 w = 0.9810489799842499 b = 46.00208012557451\n",
            "iteration 890 loss = 22000.897956921155 w = 0.9811517868700704 b = 46.0020812718653\n",
            "iteration 891 loss = 21989.373043888296 w = 0.9812544505333168 b = 46.002082416461015\n",
            "iteration 892 loss = 21977.904785568404 w = 0.9813569711735174 b = 46.00208355936402\n",
            "iteration 893 loss = 21966.492989993134 w = 0.9814593489899223 b = 46.00208470057667\n",
            "iteration 894 loss = 21955.137465776243 w = 0.9815615841815041 b = 46.002085840101316\n",
            "iteration 895 loss = 21943.838022112093 w = 0.9816636769469583 b = 46.002086977940316\n",
            "iteration 896 loss = 21932.594468773743 w = 0.9817656274847036 b = 46.00208811409602\n",
            "iteration 897 loss = 21921.40661611147 w = 0.9818674359928822 b = 46.00208924857076\n",
            "iteration 898 loss = 21910.274275050946 w = 0.9819691026693601 b = 46.0020903813669\n",
            "iteration 899 loss = 21899.197257091666 w = 0.982070627711728 b = 46.00209151248676\n",
            "iteration 900 loss = 21888.175374305247 w = 0.9821720113173011 b = 46.00209264193268\n",
            "iteration 901 loss = 21877.20843933376 w = 0.9822732536831198 b = 46.00209376970699\n",
            "iteration 902 loss = 21866.296265388148 w = 0.9823743550059499 b = 46.00209489581202\n",
            "iteration 903 loss = 21855.438666246435 w = 0.9824753154822832 b = 46.00209602025011\n",
            "iteration 904 loss = 21844.63545625221 w = 0.9825761353083377 b = 46.00209714302356\n",
            "iteration 905 loss = 21833.88645031297 w = 0.9826768146800579 b = 46.0020982641347\n",
            "iteration 906 loss = 21823.191463898365 w = 0.9827773537931156 b = 46.00209938358585\n",
            "iteration 907 loss = 21812.550313038777 w = 0.9828777528429098 b = 46.00210050137931\n",
            "iteration 908 loss = 21801.962814323506 w = 0.9829780120245674 b = 46.0021016175174\n",
            "iteration 909 loss = 21791.428784899217 w = 0.9830781315329433 b = 46.00210273200242\n",
            "iteration 910 loss = 21780.948042468328 w = 0.9831781115626212 b = 46.002103844836675\n",
            "iteration 911 loss = 21770.520405287425 w = 0.9832779523079136 b = 46.002104956022464\n",
            "iteration 912 loss = 21760.14569216557 w = 0.9833776539628621 b = 46.00210606556209\n",
            "iteration 913 loss = 21749.823722462785 w = 0.9834772167212383 b = 46.00210717345784\n",
            "iteration 914 loss = 21739.55431608843 w = 0.9835766407765437 b = 46.002108279712004\n",
            "iteration 915 loss = 21729.337293499557 w = 0.9836759263220101 b = 46.00210938432687\n",
            "iteration 916 loss = 21719.172475699437 w = 0.9837750735506002 b = 46.002110487304726\n",
            "iteration 917 loss = 21709.0596842358 w = 0.9838740826550081 b = 46.00211158864784\n",
            "iteration 918 loss = 21698.998741199415 w = 0.983972953827659 b = 46.00211268835851\n",
            "iteration 919 loss = 21688.989469222466 w = 0.9840716872607105 b = 46.00211378643899\n",
            "iteration 920 loss = 21679.03169147692 w = 0.984170283146052 b = 46.00211488289156\n",
            "iteration 921 loss = 21669.12523167302 w = 0.9842687416753062 b = 46.00211597771849\n",
            "iteration 922 loss = 21659.269914057735 w = 0.9843670630398282 b = 46.00211707092204\n",
            "iteration 923 loss = 21649.465563413185 w = 0.984465247430707 b = 46.00211816250447\n",
            "iteration 924 loss = 21639.712005054975 w = 0.984563295038765 b = 46.00211925246805\n",
            "iteration 925 loss = 21630.009064830916 w = 0.9846612060545592 b = 46.00212034081502\n",
            "iteration 926 loss = 21620.356569119172 w = 0.9847589806683807 b = 46.00212142754764\n",
            "iteration 927 loss = 21610.754344826917 w = 0.9848566190702558 b = 46.00212251266816\n",
            "iteration 928 loss = 21601.202219388742 w = 0.9849541214499459 b = 46.00212359617882\n",
            "iteration 929 loss = 21591.700020765158 w = 0.985051487996948 b = 46.00212467808187\n",
            "iteration 930 loss = 21582.24757744093 w = 0.9851487189004953 b = 46.00212575837955\n",
            "iteration 931 loss = 21572.844718423745 w = 0.9852458143495572 b = 46.00212683707409\n",
            "iteration 932 loss = 21563.49127324261 w = 0.9853427745328399 b = 46.00212791416772\n",
            "iteration 933 loss = 21554.187071946282 w = 0.9854395996387868 b = 46.00212898966269\n",
            "iteration 934 loss = 21544.931945101882 w = 0.9855362898555786 b = 46.002130063561204\n",
            "iteration 935 loss = 21535.72572379318 w = 0.985632845371134 b = 46.0021311358655\n",
            "iteration 936 loss = 21526.568239619395 w = 0.9857292663731099 b = 46.00213220657779\n",
            "iteration 937 loss = 21517.459324693395 w = 0.9858255530489015 b = 46.0021332757003\n",
            "iteration 938 loss = 21508.398811640436 w = 0.9859217055856433 b = 46.00213434323524\n",
            "iteration 939 loss = 21499.386533596553 w = 0.9860177241702088 b = 46.002135409184824\n",
            "iteration 940 loss = 21490.42232420712 w = 0.9861136089892114 b = 46.00213647355126\n",
            "iteration 941 loss = 21481.506017625266 w = 0.9862093602290043 b = 46.002137536336754\n",
            "iteration 942 loss = 21472.637448510526 w = 0.9863049780756813 b = 46.00213859754351\n",
            "iteration 943 loss = 21463.816452027408 w = 0.9864004627150768 b = 46.00213965717372\n",
            "iteration 944 loss = 21455.042863843704 w = 0.9864958143327662 b = 46.002140715229594\n",
            "iteration 945 loss = 21446.316520129265 w = 0.9865910331140668 b = 46.00214177171331\n",
            "iteration 946 loss = 21437.637257554477 w = 0.9866861192440373 b = 46.00214282662707\n",
            "iteration 947 loss = 21429.004913288587 w = 0.9867810729074786 b = 46.002143879973055\n",
            "iteration 948 loss = 21420.419324998635 w = 0.9868758942889346 b = 46.00214493175345\n",
            "iteration 949 loss = 21411.88033084771 w = 0.9869705835726916 b = 46.002145981970436\n",
            "iteration 950 loss = 21403.387769493616 w = 0.9870651409427794 b = 46.00214703062619\n",
            "iteration 951 loss = 21394.941480087433 w = 0.9871595665829713 b = 46.002148077722886\n",
            "iteration 952 loss = 21386.5413022721 w = 0.9872538606767848 b = 46.002149123262704\n",
            "iteration 953 loss = 21378.18707618087 w = 0.9873480234074814 b = 46.002150167247805\n",
            "iteration 954 loss = 21369.878642436062 w = 0.9874420549580676 b = 46.002151209680356\n",
            "iteration 955 loss = 21361.615842147465 w = 0.9875359555112948 b = 46.002152250562524\n",
            "iteration 956 loss = 21353.398516911042 w = 0.9876297252496599 b = 46.00215328989646\n",
            "iteration 957 loss = 21345.22650880752 w = 0.9877233643554053 b = 46.00215432768433\n",
            "iteration 958 loss = 21337.09966040085 w = 0.9878168730105199 b = 46.00215536392829\n",
            "iteration 959 loss = 21329.01781473689 w = 0.9879102513967388 b = 46.00215639863048\n",
            "iteration 960 loss = 21320.980815342034 w = 0.988003499695544 b = 46.00215743179306\n",
            "iteration 961 loss = 21312.9885062218 w = 0.9880966180881647 b = 46.00215846341816\n",
            "iteration 962 loss = 21305.04073185932 w = 0.9881896067555775 b = 46.00215949350794\n",
            "iteration 963 loss = 21297.137337214182 w = 0.988282465878507 b = 46.00216052206452\n",
            "iteration 964 loss = 21289.278167720706 w = 0.9883751956374259 b = 46.002161549090054\n",
            "iteration 965 loss = 21281.4630692869 w = 0.9884677962125558 b = 46.00216257458666\n",
            "iteration 966 loss = 21273.69188829297 w = 0.9885602677838666 b = 46.00216359855648\n",
            "iteration 967 loss = 21265.964471589778 w = 0.9886526105310782 b = 46.00216462100163\n",
            "iteration 968 loss = 21258.280666497638 w = 0.9887448246336598 b = 46.00216564192424\n",
            "iteration 969 loss = 21250.640320805083 w = 0.9888369102708303 b = 46.002166661326434\n",
            "iteration 970 loss = 21243.043282767103 w = 0.9889288676215594 b = 46.002167679210324\n",
            "iteration 971 loss = 21235.489401104216 w = 0.9890206968645672 b = 46.00216869557803\n",
            "iteration 972 loss = 21227.97852500076 w = 0.9891123981783249 b = 46.00216971043166\n",
            "iteration 973 loss = 21220.51050410377 w = 0.9892039717410549 b = 46.002170723773325\n",
            "iteration 974 loss = 21213.085188521556 w = 0.9892954177307317 b = 46.00217173560513\n",
            "iteration 975 loss = 21205.70242882244 w = 0.9893867363250815 b = 46.00217274592919\n",
            "iteration 976 loss = 21198.362076033194 w = 0.9894779277015829 b = 46.00217375474758\n",
            "iteration 977 loss = 21191.063981637904 w = 0.9895689920374676 b = 46.002174762062424\n",
            "iteration 978 loss = 21183.807997576583 w = 0.98965992950972 b = 46.002175767875805\n",
            "iteration 979 loss = 21176.5939762439 w = 0.9897507402950783 b = 46.00217677218981\n",
            "iteration 980 loss = 21169.421770487723 w = 0.989841424570034 b = 46.00217777500654\n",
            "iteration 981 loss = 21162.291233607917 w = 0.9899319825108331 b = 46.002178776328066\n",
            "iteration 982 loss = 21155.202219354884 w = 0.9900224142934761 b = 46.00217977615648\n",
            "iteration 983 loss = 21148.15458192845 w = 0.9901127200937182 b = 46.002180774493866\n",
            "iteration 984 loss = 21141.148175976436 w = 0.9902029000870696 b = 46.00218177134229\n",
            "iteration 985 loss = 21134.182856593394 w = 0.9902929544487961 b = 46.00218276670385\n",
            "iteration 986 loss = 21127.258479319153 w = 0.9903828833539193 b = 46.002183760580586\n",
            "iteration 987 loss = 21120.3749001378 w = 0.9904726869772171 b = 46.002184752974586\n",
            "iteration 988 loss = 21113.531975476133 w = 0.9905623654932239 b = 46.00218574388791\n",
            "iteration 989 loss = 21106.72956220249 w = 0.9906519190762307 b = 46.00218673332262\n",
            "iteration 990 loss = 21099.967517625464 w = 0.9907413479002861 b = 46.00218772128078\n",
            "iteration 991 loss = 21093.245699492556 w = 0.9908306521391959 b = 46.00218870776444\n",
            "iteration 992 loss = 21086.56396598897 w = 0.9909198319665238 b = 46.00218969277566\n",
            "iteration 993 loss = 21079.922175736305 w = 0.9910088875555918 b = 46.00219067631649\n",
            "iteration 994 loss = 21073.320187791214 w = 0.9910978190794804 b = 46.00219165838898\n",
            "iteration 995 loss = 21066.75786164423 w = 0.991186626711029 b = 46.002192638995176\n",
            "iteration 996 loss = 21060.235057218546 w = 0.9912753106228361 b = 46.00219361813711\n",
            "iteration 997 loss = 21053.751634868535 w = 0.9913638709872601 b = 46.00219459581684\n",
            "iteration 998 loss = 21047.307455378785 w = 0.9914523079764187 b = 46.00219557203639\n",
            "iteration 999 loss = 21040.9023799625 w = 0.9915406217621904 b = 46.0021965467978\n",
            "iteration 1000 loss = 21034.536270260614 w = 0.9916288125162138 b = 46.002197520103095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = w_his[999]\n",
        "b = b_his[999]\n",
        "y_predicted = [w*row[0]+b for row in validation_data]\n",
        "y_actual = [row[1] for row in validation_data]"
      ],
      "metadata": {
        "id": "4gqoUcfJhMzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_sum = 0\n",
        "for i in range(len(y_predicted)):\n",
        "  error_sum += abs((y_actual[i] - y_predicted[i])/y_actual[i])\n",
        "mape = 1/len(y_predicted) * error_sum\n",
        "print(mape*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFFoe7f-hu7_",
        "outputId": "5e21cebb-f17d-48fd-db0f-72e58e071478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.263649218957151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this\n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_datalist = []"
      ],
      "metadata": {
        "id": "Y0BOZSI7FqGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "prediction_x = [eval(row[0]) for row in testing_datalist[1:]]\n",
        "prediction_y = [w*x+b for x in prediction_x]\n",
        "for data in prediction_y:\n",
        "  output_datalist.append([data])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90EisOc7YG-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f9396b-f08b-4985-9a19-05eb197cc69b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9916288125162138 46.002197520103095\n"
          ]
        }
      ],
      "source": [
        "print(f'{w} {b}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in coefficient_output:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvlHtfSykp2_"
      },
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "  training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "  testing_datalist = np.array(list(csv.reader(csvfile)))"
      ],
      "metadata": {
        "id": "oJxJxbxKt5AQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace missing data with NaN value"
      ],
      "metadata": {
        "id": "foTzpohn3wiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_datalist[:, :] = np.where(training_datalist[:, :] == '', np.nan, training_datalist[:, :])"
      ],
      "metadata": {
        "id": "1yBJfbR13hej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split training data into multiple patients' dataset"
      ],
      "metadata": {
        "id": "XAp3p-lwKa4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PreprocessData(data):\n",
        "    # Fill NaN values with median\n",
        "    medians = np.nanmedian(data[:, 2:].astype(float), axis=0)\n",
        "    for col_idx in range(2, data.shape[1]-1):\n",
        "        nan_idx = np.isnan(data[:, col_idx].astype(float))\n",
        "        data[nan_idx, col_idx] = medians[col_idx-2]\n",
        "\n",
        "    iqr = np.percentile(np.array(data[:,2:]).astype(float), 75, axis=0) - np.percentile(np.array(data[:,2:]).astype(float), 25, axis=0)\n",
        "\n",
        "    lower_bound = np.percentile(np.array(data[:, 2:]).astype(float), 25, axis=0) - 1.5 * iqr.astype(float)\n",
        "    upper_bound = np.percentile(np.array(data[:, 2:]).astype(float), 75, axis=0) + 1.5 * iqr.astype(float)\n",
        "    outlier_mask = np.any((np.array(data[:, 2:]).astype(float) < lower_bound) | (np.array(data[:,2:]).astype(float) > upper_bound), axis=1)\n",
        "\n",
        "    return data[~outlier_mask]"
      ],
      "metadata": {
        "id": "Ezc4pg5V6cAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = PreprocessData(training_datalist[1:,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpcZCimUfYJJ",
        "outputId": "f014da07-d3ef-4b8c-89e6-3b55a03ba0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 98.  86.  17.  98. 129.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient_1 = training_data[training_data[:, 0] == '11526383', 2:]\n",
        "patient_2 = training_data[training_data[:, 0] == '12923910', 2:]\n",
        "patient_3 = training_data[training_data[:, 0] == '14699420', 2:]\n",
        "patient_4 = training_data[training_data[:, 0] == '15437705', 2:]\n",
        "patient_5 = training_data[training_data[:, 0] == '15642911', 2:]\n",
        "patient_6 = training_data[training_data[:, 0] == '16298357', 2:]\n",
        "patient_7 = training_data[training_data[:, 0] == '17331999', 2:]\n",
        "patient_8 = training_data[training_data[:, 0] == '17593883', 2:]\n",
        "patient_9 = training_data[training_data[:, 0] == '18733920', 2:]\n",
        "patient_10 = training_data[training_data[:, 0] == '18791093', 2:]\n",
        "patient_11 = training_data[training_data[:, 0] == '19473413', 2:]"
      ],
      "metadata": {
        "id": "OjX2aXua4AvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_1_testing = testing_datalist[testing_datalist[:, 0] == '11526383', 2:]\n",
        "patient_2_testing = testing_datalist[testing_datalist[:, 0] == '12923910', 2:]\n",
        "patient_3_testing = testing_datalist[testing_datalist[:, 0] == '14699420', 2:]\n",
        "patient_4_testing = testing_datalist[testing_datalist[:, 0] == '15437705', 2:]\n",
        "patient_5_testing = testing_datalist[testing_datalist[:, 0] == '15642911', 2:]\n",
        "patient_6_testing = testing_datalist[testing_datalist[:, 0] == '16298357', 2:]\n",
        "patient_7_testing = testing_datalist[testing_datalist[:, 0] == '17331999', 2:]\n",
        "patient_8_testing = testing_datalist[testing_datalist[:, 0] == '17593883', 2:]\n",
        "patient_9_testing = testing_datalist[testing_datalist[:, 0] == '18733920', 2:]\n",
        "patient_10_testing = testing_datalist[testing_datalist[:, 0] == '18791093', 2:]\n",
        "patient_11_testing = testing_datalist[testing_datalist[:, 0] == '19473413', 2:]"
      ],
      "metadata": {
        "id": "MgFR3k_JhC0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71L-05_Ukp2_"
      },
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SplitData(data):\n",
        "  np.random.shuffle(data)\n",
        "  training_data = data[:int((len(data)+1)*.75)]\n",
        "  validation_data = data[int((len(data)+1)*.75):]\n",
        "  return training_data, validation_data"
      ],
      "metadata": {
        "id": "p2jBHrf2M8tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MatrixInversion(data):\n",
        "  y = np.array([eval(row[4]) for row in data])\n",
        "  x = np.array([[1, eval(row[0]), eval(row[1]), eval(row[2]), eval(row[3])] for row in data])\n",
        "  x_transpose = x.transpose()\n",
        "  x_xT = np.matmul(x_transpose, x)\n",
        "  x_inv = inv(x_xT)\n",
        "  x_inv_xT = np.matmul(x_inv, x_transpose)\n",
        "  beta = np.matmul(x_inv_xT, y)\n",
        "  return beta\n"
      ],
      "metadata": {
        "id": "srlRw8t14uWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CalculateMAPE(data, beta):\n",
        "  predicted_y = [beta[0] + beta[1]*eval(row[0]) + beta[2]*eval(row[1]) + beta[3]*eval(row[2]) + beta[4]*eval(row[3]) for row in data]\n",
        "  actual_y = [eval(row[4]) for row in data]\n",
        "  error_sum = 0\n",
        "  for i in range(len(predicted_y)):\n",
        "    error_sum += abs((actual_y[i] - predicted_y[i])/actual_y[i])\n",
        "  mape = 1/len(predicted_y) * error_sum\n",
        "  return mape*100"
      ],
      "metadata": {
        "id": "diGSz3x4cdxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient1_training, patient1_validation = SplitData(patient_1)\n",
        "patient2_training, patient2_validation = SplitData(patient_2)\n",
        "patient3_training, patient3_validation = SplitData(patient_3)\n",
        "patient4_training, patient4_validation = SplitData(patient_4)\n",
        "patient5_training, patient5_validation = SplitData(patient_5)\n",
        "patient6_training, patient6_validation = SplitData(patient_6)\n",
        "patient7_training, patient7_validation = SplitData(patient_7)\n",
        "patient8_training, patient8_validation = SplitData(patient_8)\n",
        "patient9_training, patient9_validation = SplitData(patient_9)\n",
        "patient10_training, patient10_validation = SplitData(patient_10)\n",
        "patient11_training, patient11_validation = SplitData(patient_11)\n",
        "beta1 = MatrixInversion(patient1_training)\n",
        "beta2 = MatrixInversion(patient2_training)\n",
        "beta3 = MatrixInversion(patient3_training)\n",
        "beta4 = MatrixInversion(patient4_training)\n",
        "beta5 = MatrixInversion(patient5_training)\n",
        "beta6 = MatrixInversion(patient6_training)\n",
        "beta7 = MatrixInversion(patient7_training)\n",
        "beta8 = MatrixInversion(patient8_training)\n",
        "beta9 = MatrixInversion(patient9_training)\n",
        "beta10 = MatrixInversion(patient10_training)\n",
        "beta11 = MatrixInversion(patient11_training)\n",
        "mape1 = CalculateMAPE(patient1_validation, beta1)\n",
        "mape2 = CalculateMAPE(patient2_validation, beta2)\n",
        "mape3 = CalculateMAPE(patient3_validation, beta3)\n",
        "mape4 = CalculateMAPE(patient4_validation, beta4)\n",
        "mape5 = CalculateMAPE(patient5_validation, beta5)\n",
        "mape6 = CalculateMAPE(patient6_validation, beta6)\n",
        "mape7 = CalculateMAPE(patient7_validation, beta7)\n",
        "mape8 = CalculateMAPE(patient8_validation, beta8)\n",
        "mape9 = CalculateMAPE(patient9_validation, beta9)\n",
        "mape10 = CalculateMAPE(patient10_validation, beta10)\n",
        "mape11 = CalculateMAPE(patient11_validation, beta11)\n",
        "mape_avg = (mape1 + mape2 + mape3 + mape4 + mape5 + mape6 + mape7 +mape8 + mape9 + mape10 + mape11)/11\n",
        "mape_avg"
      ],
      "metadata": {
        "id": "PP2M3ioEkOJQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eda866d-f075-4810-f07d-c55578252555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.414629790195388"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MakePrediction(beta, testing_data):\n",
        "  prediction = beta[0] + beta[1] * testing_data[:,0].astype(float) + beta[2] * testing_data[:,1].astype(float) + beta[3] * testing_data[:, 2].astype(float) + beta[4] * testing_data[:, 3].astype(float)\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "TcHPfTwqggen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_1_prediction = MakePrediction(beta1, patient_1_testing)\n",
        "patient_2_prediction = MakePrediction(beta2, patient_2_testing)\n",
        "patient_3_prediction = MakePrediction(beta3, patient_3_testing)\n",
        "patient_4_prediction = MakePrediction(beta4, patient_4_testing)\n",
        "patient_5_prediction = MakePrediction(beta5, patient_5_testing)\n",
        "patient_6_prediction = MakePrediction(beta6, patient_6_testing)\n",
        "patient_7_prediction = MakePrediction(beta7, patient_7_testing)\n",
        "patient_8_prediction = MakePrediction(beta8, patient_8_testing)\n",
        "patient_9_prediction = MakePrediction(beta9, patient_9_testing)\n",
        "patient_10_prediction = MakePrediction(beta10, patient_10_testing)\n",
        "patient_11_prediction = MakePrediction(beta11, patient_11_testing)"
      ],
      "metadata": {
        "id": "xtAI4QV1lGuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in patient_1_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_2_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_3_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_4_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_5_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_6_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_7_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_8_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_9_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_10_prediction:\n",
        "  output_datalist.append([i])\n",
        "for i in patient_11_prediction:\n",
        "  output_datalist.append([i])\n"
      ],
      "metadata": {
        "id": "dF6WUj5Fmo89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEdNBT3Qkp2_"
      },
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGHZKiv7kp2_"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
